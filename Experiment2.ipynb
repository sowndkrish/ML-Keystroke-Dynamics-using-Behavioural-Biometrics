{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UUID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UUID\n",
       "0    48\n",
       "1    48\n",
       "2    48\n",
       "3    48\n",
       "4    48"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aLN1</th>\n",
       "      <th>a.2</th>\n",
       "      <th>aLN3</th>\n",
       "      <th>at4</th>\n",
       "      <th>ai5</th>\n",
       "      <th>ae6</th>\n",
       "      <th>aLN7</th>\n",
       "      <th>a58</th>\n",
       "      <th>aLN9</th>\n",
       "      <th>aSH10</th>\n",
       "      <th>...</th>\n",
       "      <th>du2o12</th>\n",
       "      <th>du2a13</th>\n",
       "      <th>du2n14</th>\n",
       "      <th>du2n15</th>\n",
       "      <th>avgdu</th>\n",
       "      <th>avgud</th>\n",
       "      <th>avgdd</th>\n",
       "      <th>avguu</th>\n",
       "      <th>avdu2</th>\n",
       "      <th>avga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2382</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>973</td>\n",
       "      <td>37.875</td>\n",
       "      <td>24.466667</td>\n",
       "      <td>56.800000</td>\n",
       "      <td>55.866667</td>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.004412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>3015</td>\n",
       "      <td>1081</td>\n",
       "      <td>37.625</td>\n",
       "      <td>31.933333</td>\n",
       "      <td>64.066667</td>\n",
       "      <td>63.266667</td>\n",
       "      <td>95.400000</td>\n",
       "      <td>0.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3015</td>\n",
       "      <td>2361</td>\n",
       "      <td>1918</td>\n",
       "      <td>884</td>\n",
       "      <td>64.125</td>\n",
       "      <td>453.733333</td>\n",
       "      <td>515.933333</td>\n",
       "      <td>513.133333</td>\n",
       "      <td>575.333333</td>\n",
       "      <td>0.008333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2361</td>\n",
       "      <td>1918</td>\n",
       "      <td>1438</td>\n",
       "      <td>827</td>\n",
       "      <td>63.250</td>\n",
       "      <td>347.733333</td>\n",
       "      <td>407.733333</td>\n",
       "      <td>406.400000</td>\n",
       "      <td>466.400000</td>\n",
       "      <td>0.008211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2382</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>973</td>\n",
       "      <td>69.375</td>\n",
       "      <td>-9.133333</td>\n",
       "      <td>56.800000</td>\n",
       "      <td>55.866667</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>0.009804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aLN1       a.2      aLN3       at4       ai5       ae6      aLN7  a58  \\\n",
       "0  0.017647  0.015686  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "1  0.015686  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "2  0.017647  0.015686  0.015686  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "3  0.015686  0.015686  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "4  0.017647  0.015686  0.015686  0.019608  0.015686  0.017647  0.017647  0.0   \n",
       "\n",
       "   aLN9  aSH10    ...     du2o12     du2a13     du2n14  du2n15   avgdu  \\\n",
       "0   0.0    0.0    ...       2382       2302  670740857     973  37.875   \n",
       "1   0.0    0.0    ...       2302  670740857       3015    1081  37.625   \n",
       "2   0.0    0.0    ...       3015       2361       1918     884  64.125   \n",
       "3   0.0    0.0    ...       2361       1918       1438     827  63.250   \n",
       "4   0.0    0.0    ...       2382       2302  670740857     973  69.375   \n",
       "\n",
       "        avgud       avgdd       avguu       avdu2      avga  \n",
       "0   24.466667   56.800000   55.866667   88.200000  0.004412  \n",
       "1   31.933333   64.066667   63.266667   95.400000  0.004167  \n",
       "2  453.733333  515.933333  513.133333  575.333333  0.008333  \n",
       "3  347.733333  407.733333  406.400000  466.400000  0.008211  \n",
       "4   -9.133333   56.800000   55.866667  121.800000  0.009804  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2310 entries, 0 to 2309\n",
      "Columns: 130 entries, aLN1 to avga\n",
      "dtypes: float64(54), int64(76)\n",
      "memory usage: 2.3 MB\n",
      "initial data info None\n",
      "data is (2310, 130)\n",
      "(2310, 130)\n",
      "(2310,)\n",
      "130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  3\n",
      "f1  0.21366203718807766\n",
      "Accuracy: 0.2722943722943723\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  4\n",
      "f1  0.4668496126033941\n",
      "Accuracy: 0.5\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  5\n",
      "f1  0.48122783486630133\n",
      "Accuracy: 0.5125541125541125\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  6\n",
      "f1  0.474068722922489\n",
      "Accuracy: 0.5043290043290043\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  7\n",
      "f1  0.5318955035448361\n",
      "Accuracy: 0.554978354978355\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  8\n",
      "f1  0.5395534558282092\n",
      "Accuracy: 0.5632034632034632\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  9\n",
      "f1  0.5306624396200694\n",
      "Accuracy: 0.5536796536796537\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  10\n",
      "f1  0.5477364646732952\n",
      "Accuracy: 0.5696969696969696\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  11\n",
      "f1  0.5478326109002258\n",
      "Accuracy: 0.5696969696969697\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  12\n",
      "f1  0.5580668164476151\n",
      "Accuracy: 0.5774891774891775\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  13\n",
      "f1  0.5586109225561912\n",
      "Accuracy: 0.5783549783549783\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  14\n",
      "f1  0.560662249811213\n",
      "Accuracy: 0.580952380952381\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  15\n",
      "f1  0.5562276949973027\n",
      "Accuracy: 0.5766233766233766\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  16\n",
      "f1  0.5612682006646708\n",
      "Accuracy: 0.5800865800865801\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  17\n",
      "f1  0.558733274034984\n",
      "Accuracy: 0.577056277056277\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  18\n",
      "f1  0.560651440750706\n",
      "Accuracy: 0.5792207792207792\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  19\n",
      "f1  0.570444435246132\n",
      "Accuracy: 0.5874458874458874\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  20\n",
      "f1  0.5783682286178733\n",
      "Accuracy: 0.5930735930735931\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  21\n",
      "f1  0.5784146998446349\n",
      "Accuracy: 0.5930735930735931\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  22\n",
      "f1  0.5908941235930097\n",
      "Accuracy: 0.6012987012987013\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  23\n",
      "f1  0.6227611286537346\n",
      "Accuracy: 0.6303030303030304\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  24\n",
      "f1  0.6222802098884319\n",
      "Accuracy: 0.6303030303030303\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  25\n",
      "f1  0.6246383207530363\n",
      "Accuracy: 0.6316017316017316\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  26\n",
      "f1  0.620915111458042\n",
      "Accuracy: 0.629004329004329\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  27\n",
      "f1  0.6279050953934873\n",
      "Accuracy: 0.6359307359307359\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  28\n",
      "f1  0.6267663015212759\n",
      "Accuracy: 0.6354978354978356\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  29\n",
      "f1  0.624945496488095\n",
      "Accuracy: 0.6329004329004329\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  30\n",
      "f1  0.6289056049735543\n",
      "Accuracy: 0.6363636363636364\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  31\n",
      "f1  0.6300751484425631\n",
      "Accuracy: 0.6376623376623376\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  32\n",
      "f1  0.6394423198993167\n",
      "Accuracy: 0.6463203463203463\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  33\n",
      "f1  0.6487112965215978\n",
      "Accuracy: 0.6571428571428571\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  34\n",
      "f1  0.6447006640774873\n",
      "Accuracy: 0.6545454545454545\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  35\n",
      "f1  0.6462680879152259\n",
      "Accuracy: 0.6554112554112554\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  36\n",
      "f1  0.6493397202398556\n",
      "Accuracy: 0.658874458874459\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  37\n",
      "f1  0.6444694532430154\n",
      "Accuracy: 0.6528138528138528\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  38\n",
      "f1  0.6477758233680498\n",
      "Accuracy: 0.6549783549783549\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  39\n",
      "f1  0.650567473637627\n",
      "Accuracy: 0.6571428571428573\n",
      "metrics\n",
      "metrics\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import lsanomaly\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn import utils  \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "\n",
    "# import the CSV from http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "# this will return a pandas dataframe.\n",
    "data = pd.read_csv('C:/Users/S/Documents/PY/increased30featureswopressure.csv', low_memory=False)\n",
    "'''data.loc[data['UUID'] == \"RVTNB1502866560357\", \"attack\"] = 1  \n",
    "data.loc[data['UUID'] != \"RVTNB1502866560357\", \"attack\"] = -1\n",
    "df_majority = data[data['attack']==-1]\n",
    "df_minority = data[data['attack']==1]\n",
    "from sklearn.utils import resample\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=830,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "data = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "#print(data['attack'].value_counts())'''\n",
    "\n",
    "#target=np.array(target)\n",
    "#target = pd.DataFrame(target,columns=['attack'])\n",
    "\n",
    "#data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "categorical_columns=[\"UUID\"]\n",
    "cate_data = data[categorical_columns]\n",
    "\n",
    "#for col in data.columns.values:\n",
    "#    print(col, data[col].unique())\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "def label_encode(cate_data, columns):\n",
    "    for col in columns:\n",
    "        le = LabelEncoder()\n",
    "        col_values_unique = list(cate_data[col].unique())\n",
    "        le_fitted = le.fit(col_values_unique)\n",
    " \n",
    "        col_values = list(cate_data[col].values)\n",
    "        le.classes_\n",
    "        col_values_transformed = le.transform(col_values)\n",
    "        cate_data[col] = col_values_transformed\n",
    " \n",
    "to_be_encoded_cols = cate_data.columns.values\n",
    "label_encode(cate_data, to_be_encoded_cols)\n",
    "display(cate_data.head())\n",
    "target=cate_data['UUID']\n",
    "target=np.array(target)\n",
    "#target = pd.DataFrame(target)\n",
    "#target=target1.values\n",
    "\n",
    "data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "data=pd.concat([data,cate_data], axis=1)\n",
    "data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "\n",
    "# check the shape for sanity checking.\n",
    "data.shape\n",
    "display(data.head())\n",
    "print(\"initial data info\",data.info())\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"data is\",data.shape)\n",
    "from skfeature.function.information_theoretical_based import LCSI\n",
    "from skfeature.function.information_theoretical_based import MRMR\n",
    "\n",
    "from skfeature.utility.entropy_estimators import *\n",
    "import scipy.io\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#scaled_data=data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "scaleddata= pd.DataFrame(scaled_data)\n",
    "scaled_data=np.array(scaled_data)\n",
    "\n",
    "print(scaled_data.shape)\n",
    "print(target.shape)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "#display(target.head())\n",
    "#idx=MRMR.mrmr(scaled_data,target,n_selected_features=50)\n",
    "'''from sklearn import cross_validation\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "correct = 0\n",
    "print(\"scaled data details - \",scaled_data.info())\n",
    "print(\"target data details - \",target.info())\n",
    "for train, test in ss:\n",
    "    #print(scaled_data[train])\n",
    "    #print(target[train])\n",
    "        # obtain the index of each feature on the training set\n",
    "    idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=50)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "    features = scaled_data[:, idx[0:50]]\n",
    "print(features)    '''\n",
    "'''skb= SVC(kernel=\"linear\")\n",
    "rfe = RFE(estimator=skb, n_features_to_select=70)\n",
    "rfe=rfe.fit(scaleddata,target)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "skft = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "for train, test in skft:\n",
    "    X_train,X_test=scaled_data.iloc[train],scaled_data.iloc[test]\n",
    "    Y_train,y_test=target.iloc[train],target.iloc[test]\n",
    "    model1 = svm.OneClassSVM(nu=nu, kernel='rbf', gamma=0.10000000000000001)  \n",
    "    model1.fit(X_train, Y_train)\n",
    "    scores = cross_val_score(model1,X_test,y_test, cv=5, scoring='accuracy')\n",
    "    print(scores)\n",
    "print(scores.mean())'''\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "#rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5,random_state=36851234)\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "clf=SVC(kernel='linear')\n",
    "#clf = svm.SVC(decision_function_shape='ovo')    # linear SVM\n",
    "correct = 0\n",
    "fscoreTotal =0\n",
    "print(scaled_data.shape[1])\n",
    "plt.figure(figsize=(16, 8))\n",
    "accuracy = plt.subplot(221)\n",
    "\n",
    "x=np.array([])\n",
    "y=np.array([])\n",
    "f1val=np.array([])\n",
    "numoffeatures= lambda start, end: range(start, end+1)\n",
    "for i in numoffeatures(3,scaled_data.shape[1]):\n",
    "    for train, test in skf.split(scaled_data,target):\n",
    "        # obtain the index of each feature on the training set\n",
    "        idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=i)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "        features = scaled_data[:, idx[0:i]]\n",
    "        #print(target[train])\n",
    "        # train a classification model with the selected features on the training dataset\n",
    "        clf.fit(features[train], target[train])\n",
    "\n",
    "        # predict the class labels of test data\n",
    "        y_predict = clf.predict(features[test])\n",
    "        print(\"metrics\")\n",
    "        # obtain the classification accuracy on the test data\n",
    "        acc = accuracy_score(target[test], y_predict)\n",
    "        correct = correct + acc\n",
    "        fscore=f1_score(target[test], y_predict,average='weighted')\n",
    "        fscoreTotal=fscoreTotal+fscore\n",
    "        #print(\"fsc \",f1_score(target[test], y_predict,average='weighted'))\n",
    "        #print(\"conf mat \",confusion_matrix(target[test],y_predict))\n",
    "        #print(\"ACCURACY: \", (accuracy_score(target[test], y_predict)))\n",
    "        #report = classification_report(target[test], y_predict)\n",
    "        #print(report)\n",
    "    x=np.append(x,i)\n",
    "    accscores=float(correct)/5\n",
    "    f1scores=float(fscoreTotal)/5\n",
    "    y=np.append(y,accscores)\n",
    "    f1val=np.append(f1val,f1scores)\n",
    "    np.savetxt('exp3.txt', (y,f1val), fmt='%.5g', delimiter=',', newline='\\n')\n",
    "    print(\"loop \",i)\n",
    "    print(\"f1 \",f1scores)\n",
    "    # output the average classification accuracy over all 10 folds\n",
    "    print(\"Accuracy:\", accscores)\n",
    "    fscore=0\n",
    "    acc=0\n",
    "    correct=0\n",
    "    fscoreTotal=0\n",
    "##svc=SelectKBest(mutual_info_classif, k=50).fit_transform(data,target)\n",
    "#svc = SVC(kernel=\"linear\")\n",
    "#rfe = RFE(estimator=svc, n_features_to_select=10)\n",
    "#rfe.fit(data, target)\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, m*x + b, '-')\n",
    "accuracy.plot(x,y)\n",
    "accuracy.set_title(\"mRMR feature selection\")\n",
    "accuracy.set_xlim(3, scaled_data.shape[1])\n",
    "accuracy.set_xlabel(\"Number of Features\")\n",
    "accuracy.set_ylim(0.4, 0.8)\n",
    "accuracy.set_ylabel(\"Classification Accuracy\")\n",
    "f1=plt.subplot(222)\n",
    "n, c = np.polyfit(x, f1val, 1)\n",
    "plt.plot(x, f1val, '.')\n",
    "plt.plot(x, n*x + c, '-')\n",
    "f1.plot(x,f1val)\n",
    "f1.set_title(\"mRMR feature selection\")\n",
    "f1.set_xlim(3, scaled_data.shape[1])\n",
    "f1.set_xlabel(\"Number of Features\")\n",
    "f1.set_ylim(0.4, 0.8)\n",
    "f1.set_ylabel(\"Classification F1 Score\")\n",
    "plt.show()\n",
    "print(\"here\")\n",
    "#score = svc.score(data, target)\n",
    "##print(svc)\n",
    "#ranking = rfe.feature_importances_\n",
    "#print(\"no of feat\",ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UUID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UUID\n",
       "0    48\n",
       "1    48\n",
       "2    48\n",
       "3    48\n",
       "4    48"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aLN1</th>\n",
       "      <th>a.2</th>\n",
       "      <th>aLN3</th>\n",
       "      <th>at4</th>\n",
       "      <th>ai5</th>\n",
       "      <th>ae6</th>\n",
       "      <th>aLN7</th>\n",
       "      <th>a58</th>\n",
       "      <th>aLN9</th>\n",
       "      <th>aSH10</th>\n",
       "      <th>...</th>\n",
       "      <th>du2o12</th>\n",
       "      <th>du2a13</th>\n",
       "      <th>du2n14</th>\n",
       "      <th>du2n15</th>\n",
       "      <th>avgdu</th>\n",
       "      <th>avgud</th>\n",
       "      <th>avgdd</th>\n",
       "      <th>avguu</th>\n",
       "      <th>avdu2</th>\n",
       "      <th>avga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2382</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>973</td>\n",
       "      <td>37.875</td>\n",
       "      <td>24.466667</td>\n",
       "      <td>56.800000</td>\n",
       "      <td>55.866667</td>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.004412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>3015</td>\n",
       "      <td>1081</td>\n",
       "      <td>37.625</td>\n",
       "      <td>31.933333</td>\n",
       "      <td>64.066667</td>\n",
       "      <td>63.266667</td>\n",
       "      <td>95.400000</td>\n",
       "      <td>0.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3015</td>\n",
       "      <td>2361</td>\n",
       "      <td>1918</td>\n",
       "      <td>884</td>\n",
       "      <td>64.125</td>\n",
       "      <td>453.733333</td>\n",
       "      <td>515.933333</td>\n",
       "      <td>513.133333</td>\n",
       "      <td>575.333333</td>\n",
       "      <td>0.008333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2361</td>\n",
       "      <td>1918</td>\n",
       "      <td>1438</td>\n",
       "      <td>827</td>\n",
       "      <td>63.250</td>\n",
       "      <td>347.733333</td>\n",
       "      <td>407.733333</td>\n",
       "      <td>406.400000</td>\n",
       "      <td>466.400000</td>\n",
       "      <td>0.008211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2382</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>973</td>\n",
       "      <td>69.375</td>\n",
       "      <td>-9.133333</td>\n",
       "      <td>56.800000</td>\n",
       "      <td>55.866667</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>0.009804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aLN1       a.2      aLN3       at4       ai5       ae6      aLN7  a58  \\\n",
       "0  0.017647  0.015686  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "1  0.015686  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "2  0.017647  0.015686  0.015686  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "3  0.015686  0.015686  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "4  0.017647  0.015686  0.015686  0.019608  0.015686  0.017647  0.017647  0.0   \n",
       "\n",
       "   aLN9  aSH10    ...     du2o12     du2a13     du2n14  du2n15   avgdu  \\\n",
       "0   0.0    0.0    ...       2382       2302  670740857     973  37.875   \n",
       "1   0.0    0.0    ...       2302  670740857       3015    1081  37.625   \n",
       "2   0.0    0.0    ...       3015       2361       1918     884  64.125   \n",
       "3   0.0    0.0    ...       2361       1918       1438     827  63.250   \n",
       "4   0.0    0.0    ...       2382       2302  670740857     973  69.375   \n",
       "\n",
       "        avgud       avgdd       avguu       avdu2      avga  \n",
       "0   24.466667   56.800000   55.866667   88.200000  0.004412  \n",
       "1   31.933333   64.066667   63.266667   95.400000  0.004167  \n",
       "2  453.733333  515.933333  513.133333  575.333333  0.008333  \n",
       "3  347.733333  407.733333  406.400000  466.400000  0.008211  \n",
       "4   -9.133333   56.800000   55.866667  121.800000  0.009804  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2310 entries, 0 to 2309\n",
      "Columns: 130 entries, aLN1 to avga\n",
      "dtypes: float64(54), int64(76)\n",
      "memory usage: 2.3 MB\n",
      "initial data info None\n",
      "data is (2310, 130)\n",
      "(2310, 130)\n",
      "(2310,)\n",
      "130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  41\n",
      "f1  0.6551619922257612\n",
      "Accuracy: 0.6606060606060605\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  42\n",
      "f1  0.6590298112084529\n",
      "Accuracy: 0.6649350649350649\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  43\n",
      "f1  0.6579033103718185\n",
      "Accuracy: 0.6636363636363636\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  44\n",
      "f1  0.6573580026110791\n",
      "Accuracy: 0.6636363636363637\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  45\n",
      "f1  0.6627322585607335\n",
      "Accuracy: 0.6675324675324675\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  46\n",
      "f1  0.6633976728510523\n",
      "Accuracy: 0.6679653679653679\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  47\n",
      "f1  0.6606151564918805\n",
      "Accuracy: 0.6653679653679654\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  48\n",
      "f1  0.6627708023675565\n",
      "Accuracy: 0.6683982683982683\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  49\n",
      "f1  0.6715988863508062\n",
      "Accuracy: 0.677056277056277\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  50\n",
      "f1  0.6721330609624205\n",
      "Accuracy: 0.6774891774891774\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  51\n",
      "f1  0.6813951935287226\n",
      "Accuracy: 0.6857142857142857\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  52\n",
      "f1  0.6751020793233813\n",
      "Accuracy: 0.6792207792207792\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  53\n",
      "f1  0.6774029499971361\n",
      "Accuracy: 0.6818181818181819\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  54\n",
      "f1  0.675777153921539\n",
      "Accuracy: 0.6796536796536795\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  55\n",
      "f1  0.6750517096009839\n",
      "Accuracy: 0.677922077922078\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  56\n",
      "f1  0.6765147205244508\n",
      "Accuracy: 0.6792207792207792\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  57\n",
      "f1  0.6803835155592222\n",
      "Accuracy: 0.6831168831168831\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  58\n",
      "f1  0.6750370538061423\n",
      "Accuracy: 0.6774891774891775\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  59\n",
      "f1  0.676431683103786\n",
      "Accuracy: 0.6787878787878787\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  60\n",
      "f1  0.6770215859434978\n",
      "Accuracy: 0.6792207792207792\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  61\n",
      "f1  0.6778361173080674\n",
      "Accuracy: 0.6792207792207792\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  62\n",
      "f1  0.6762744322398888\n",
      "Accuracy: 0.6783549783549784\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  63\n",
      "f1  0.6752763983321661\n",
      "Accuracy: 0.6774891774891775\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  64\n",
      "f1  0.6758775605802263\n",
      "Accuracy: 0.6783549783549784\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  65\n",
      "f1  0.6726252175972209\n",
      "Accuracy: 0.6753246753246753\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  66\n",
      "f1  0.6647999883474847\n",
      "Accuracy: 0.6679653679653679\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  67\n",
      "f1  0.669636504917188\n",
      "Accuracy: 0.6731601731601732\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  68\n",
      "f1  0.6728715194490594\n",
      "Accuracy: 0.6774891774891775\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  69\n",
      "f1  0.6741035912846455\n",
      "Accuracy: 0.6783549783549784\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  70\n",
      "f1  0.6723240419844899\n",
      "Accuracy: 0.677922077922078\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  71\n",
      "f1  0.679646374238791\n",
      "Accuracy: 0.6848484848484848\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  72\n",
      "f1  0.6785355166726241\n",
      "Accuracy: 0.6839826839826839\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  73\n",
      "f1  0.6729694484635178\n",
      "Accuracy: 0.6792207792207792\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  74\n",
      "f1  0.6802240887655626\n",
      "Accuracy: 0.6865800865800866\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  75\n",
      "f1  0.6769946940197432\n",
      "Accuracy: 0.6835497835497836\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  76\n",
      "f1  0.6775003491319682\n",
      "Accuracy: 0.6839826839826839\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  77\n",
      "f1  0.6770814144687329\n",
      "Accuracy: 0.6831168831168831\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  78\n",
      "f1  0.6806549225897409\n",
      "Accuracy: 0.6861471861471861\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  79\n",
      "f1  0.6838065316122439\n",
      "Accuracy: 0.6891774891774892\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  80\n",
      "f1  0.6819228772915797\n",
      "Accuracy: 0.6874458874458874\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  81\n",
      "f1  0.6802317981653351\n",
      "Accuracy: 0.6878787878787879\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  82\n",
      "f1  0.6806675275367727\n",
      "Accuracy: 0.687012987012987\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  83\n",
      "f1  0.6787510181909281\n",
      "Accuracy: 0.6852813852813854\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  84\n",
      "f1  0.6775529459908419\n",
      "Accuracy: 0.6852813852813853\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  85\n",
      "f1  0.6701092503116949\n",
      "Accuracy: 0.6783549783549783\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  86\n",
      "f1  0.6701258724343844\n",
      "Accuracy: 0.6779220779220779\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  87\n",
      "f1  0.6726178028566353\n",
      "Accuracy: 0.680952380952381\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  88\n",
      "f1  0.6777737220405389\n",
      "Accuracy: 0.6852813852813853\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  89\n",
      "f1  0.6789637124521126\n",
      "Accuracy: 0.6861471861471862\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  90\n",
      "f1  0.6772668561295878\n",
      "Accuracy: 0.6848484848484848\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  91\n",
      "f1  0.6749002882401218\n",
      "Accuracy: 0.6826839826839827\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  92\n",
      "f1  0.6726519453768056\n",
      "Accuracy: 0.680952380952381\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  93\n",
      "f1  0.6705039897758325\n",
      "Accuracy: 0.6787878787878788\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  94\n",
      "f1  0.6708260664244322\n",
      "Accuracy: 0.6787878787878788\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  95\n",
      "f1  0.6684003213491373\n",
      "Accuracy: 0.6757575757575757\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  96\n",
      "f1  0.6669789195610434\n",
      "Accuracy: 0.6744588744588744\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  97\n",
      "f1  0.6666641822371387\n",
      "Accuracy: 0.674025974025974\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  98\n",
      "f1  0.6631886293588678\n",
      "Accuracy: 0.6705627705627706\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  99\n",
      "f1  0.6578898427537404\n",
      "Accuracy: 0.6662337662337663\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  100\n",
      "f1  0.6545942162412751\n",
      "Accuracy: 0.6623376623376623\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  101\n",
      "f1  0.6538200396541437\n",
      "Accuracy: 0.6627705627705628\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  102\n",
      "f1  0.6575166935939418\n",
      "Accuracy: 0.6658008658008658\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  103\n",
      "f1  0.6571088053561617\n",
      "Accuracy: 0.6653679653679654\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  104\n",
      "f1  0.6621387708918569\n",
      "Accuracy: 0.6705627705627706\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  105\n",
      "f1  0.6593168067597321\n",
      "Accuracy: 0.6683982683982684\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  106\n",
      "f1  0.6597994011760245\n",
      "Accuracy: 0.6688311688311688\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  107\n",
      "f1  0.6606501385890231\n",
      "Accuracy: 0.6692640692640693\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  108\n",
      "f1  0.6597164934818427\n",
      "Accuracy: 0.6688311688311689\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  109\n",
      "f1  0.6614719212303949\n",
      "Accuracy: 0.6701298701298701\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  110\n",
      "f1  0.6647368957048101\n",
      "Accuracy: 0.6731601731601732\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  111\n",
      "f1  0.6647027811901761\n",
      "Accuracy: 0.6727272727272726\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  112\n",
      "f1  0.6659003194704308\n",
      "Accuracy: 0.6735930735930735\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  113\n",
      "f1  0.6616987634283777\n",
      "Accuracy: 0.6701298701298701\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  114\n",
      "f1  0.6616044459588041\n",
      "Accuracy: 0.6696969696969697\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  115\n",
      "f1  0.6611970390579142\n",
      "Accuracy: 0.6696969696969697\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  116\n",
      "f1  0.6603640025869861\n",
      "Accuracy: 0.6683982683982684\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  117\n",
      "f1  0.6585636354810422\n",
      "Accuracy: 0.6666666666666667\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  118\n",
      "f1  0.6548827331561369\n",
      "Accuracy: 0.6632034632034631\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  119\n",
      "f1  0.6492323482879883\n",
      "Accuracy: 0.658874458874459\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  120\n",
      "f1  0.6493776630360533\n",
      "Accuracy: 0.658874458874459\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  121\n",
      "f1  0.6481577077939438\n",
      "Accuracy: 0.6580086580086579\n",
      "metrics\n",
      "metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  122\n",
      "f1  0.6482358229784078\n",
      "Accuracy: 0.6588744588744588\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  123\n",
      "f1  0.6464394565760343\n",
      "Accuracy: 0.6567099567099567\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  124\n",
      "f1  0.647527170456157\n",
      "Accuracy: 0.6571428571428571\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  125\n",
      "f1  0.6446387771246773\n",
      "Accuracy: 0.6545454545454545\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  126\n",
      "f1  0.644274255812868\n",
      "Accuracy: 0.6541125541125541\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  127\n",
      "f1  0.6438360921821579\n",
      "Accuracy: 0.6541125541125542\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  128\n",
      "f1  0.6458687085768827\n",
      "Accuracy: 0.6558441558441558\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  129\n",
      "f1  0.6484405610295934\n",
      "Accuracy: 0.658008658008658\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "metrics\n",
      "loop  130\n",
      "f1  0.6496966173627747\n",
      "Accuracy: 0.658874458874459\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA78AAAEDCAYAAAD9d7O9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZI09uz8SpIkSZLG3v8H5hXR0ma6I/EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc8824e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import lsanomaly\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn import utils  \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "\n",
    "# import the CSV from http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "# this will return a pandas dataframe.\n",
    "data = pd.read_csv('C:/Users/S/Documents/PY/increased30featureswopressure.csv', low_memory=False)\n",
    "'''data.loc[data['UUID'] == \"RVTNB1502866560357\", \"attack\"] = 1  \n",
    "data.loc[data['UUID'] != \"RVTNB1502866560357\", \"attack\"] = -1\n",
    "df_majority = data[data['attack']==-1]\n",
    "df_minority = data[data['attack']==1]\n",
    "from sklearn.utils import resample\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=830,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "data = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "#print(data['attack'].value_counts())'''\n",
    "\n",
    "#target=np.array(target)\n",
    "#target = pd.DataFrame(target,columns=['attack'])\n",
    "\n",
    "#data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "categorical_columns=[\"UUID\"]\n",
    "cate_data = data[categorical_columns]\n",
    "\n",
    "#for col in data.columns.values:\n",
    "#    print(col, data[col].unique())\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "def label_encode(cate_data, columns):\n",
    "    for col in columns:\n",
    "        le = LabelEncoder()\n",
    "        col_values_unique = list(cate_data[col].unique())\n",
    "        le_fitted = le.fit(col_values_unique)\n",
    " \n",
    "        col_values = list(cate_data[col].values)\n",
    "        le.classes_\n",
    "        col_values_transformed = le.transform(col_values)\n",
    "        cate_data[col] = col_values_transformed\n",
    " \n",
    "to_be_encoded_cols = cate_data.columns.values\n",
    "label_encode(cate_data, to_be_encoded_cols)\n",
    "display(cate_data.head())\n",
    "target=cate_data['UUID']\n",
    "target=np.array(target)\n",
    "#target = pd.DataFrame(target)\n",
    "#target=target1.values\n",
    "\n",
    "data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "data=pd.concat([data,cate_data], axis=1)\n",
    "data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "\n",
    "# check the shape for sanity checking.\n",
    "data.shape\n",
    "display(data.head())\n",
    "print(\"initial data info\",data.info())\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"data is\",data.shape)\n",
    "from skfeature.function.information_theoretical_based import LCSI\n",
    "from skfeature.function.information_theoretical_based import MRMR\n",
    "\n",
    "from skfeature.utility.entropy_estimators import *\n",
    "import scipy.io\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#scaled_data=data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "scaleddata= pd.DataFrame(scaled_data)\n",
    "scaled_data=np.array(scaled_data)\n",
    "\n",
    "print(scaled_data.shape)\n",
    "print(target.shape)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "#display(target.head())\n",
    "#idx=MRMR.mrmr(scaled_data,target,n_selected_features=50)\n",
    "'''from sklearn import cross_validation\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "correct = 0\n",
    "print(\"scaled data details - \",scaled_data.info())\n",
    "print(\"target data details - \",target.info())\n",
    "for train, test in ss:\n",
    "    #print(scaled_data[train])\n",
    "    #print(target[train])\n",
    "        # obtain the index of each feature on the training set\n",
    "    idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=50)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "    features = scaled_data[:, idx[0:50]]\n",
    "print(features)    '''\n",
    "'''skb= SVC(kernel=\"linear\")\n",
    "rfe = RFE(estimator=skb, n_features_to_select=70)\n",
    "rfe=rfe.fit(scaleddata,target)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "skft = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "for train, test in skft:\n",
    "    X_train,X_test=scaled_data.iloc[train],scaled_data.iloc[test]\n",
    "    Y_train,y_test=target.iloc[train],target.iloc[test]\n",
    "    model1 = svm.OneClassSVM(nu=nu, kernel='rbf', gamma=0.10000000000000001)  \n",
    "    model1.fit(X_train, Y_train)\n",
    "    scores = cross_val_score(model1,X_test,y_test, cv=5, scoring='accuracy')\n",
    "    print(scores)\n",
    "print(scores.mean())'''\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "#rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5,random_state=36851234)\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "clf=SVC(kernel='linear')\n",
    "#clf = svm.SVC(decision_function_shape='ovo')    # linear SVM\n",
    "correct = 0\n",
    "fscoreTotal =0\n",
    "print(scaled_data.shape[1])\n",
    "plt.figure(figsize=(16, 8))\n",
    "accuracy = plt.subplot(221)\n",
    "\n",
    "x=np.array([])\n",
    "y=np.array([])\n",
    "f1val=np.array([])\n",
    "numoffeatures= lambda start, end: range(start, end+1)\n",
    "for i in numoffeatures(41,scaled_data.shape[1]):\n",
    "    for train, test in skf.split(scaled_data,target):\n",
    "        # obtain the index of each feature on the training set\n",
    "        idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=i)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "        features = scaled_data[:, idx[0:i]]\n",
    "        #print(target[train])\n",
    "        # train a classification model with the selected features on the training dataset\n",
    "        clf.fit(features[train], target[train])\n",
    "\n",
    "        # predict the class labels of test data\n",
    "        y_predict = clf.predict(features[test])\n",
    "        print(\"metrics\")\n",
    "        # obtain the classification accuracy on the test data\n",
    "        acc = accuracy_score(target[test], y_predict)\n",
    "        correct = correct + acc\n",
    "        fscore=f1_score(target[test], y_predict,average='weighted')\n",
    "        fscoreTotal=fscoreTotal+fscore\n",
    "        #print(\"fsc \",f1_score(target[test], y_predict,average='weighted'))\n",
    "        #print(\"conf mat \",confusion_matrix(target[test],y_predict))\n",
    "        #print(\"ACCURACY: \", (accuracy_score(target[test], y_predict)))\n",
    "        #report = classification_report(target[test], y_predict)\n",
    "        #print(report)\n",
    "    x=np.append(x,i)\n",
    "    accscores=float(correct)/5\n",
    "    f1scores=float(fscoreTotal)/5\n",
    "    y=np.append(y,accscores)\n",
    "    f1val=np.append(f1val,f1scores)\n",
    "    np.savetxt('nxtexp3.txt', (y,f1val), fmt='%.5g', delimiter=',', newline='\\n')\n",
    "    print(\"loop \",i)\n",
    "    print(\"f1 \",f1scores)\n",
    "    # output the average classification accuracy over all 10 folds\n",
    "    print(\"Accuracy:\", accscores)\n",
    "    fscore=0\n",
    "    acc=0\n",
    "    correct=0\n",
    "    fscoreTotal=0\n",
    "##svc=SelectKBest(mutual_info_classif, k=50).fit_transform(data,target)\n",
    "#svc = SVC(kernel=\"linear\")\n",
    "#rfe = RFE(estimator=svc, n_features_to_select=10)\n",
    "#rfe.fit(data, target)\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, m*x + b, '-')\n",
    "accuracy.plot(x,y)\n",
    "accuracy.set_title(\"mRMR feature selection\")\n",
    "accuracy.set_xlim(41, scaled_data.shape[1])\n",
    "accuracy.set_xlabel(\"Number of Features\")\n",
    "accuracy.set_ylim(0.4, 0.8)\n",
    "accuracy.set_ylabel(\"Classification Accuracy\")\n",
    "f1=plt.subplot(222)\n",
    "n, c = np.polyfit(x, f1val, 1)\n",
    "plt.plot(x, f1val, '.')\n",
    "plt.plot(x, n*x + c, '-')\n",
    "f1.plot(x,f1val)\n",
    "f1.set_title(\"mRMR feature selection\")\n",
    "f1.set_xlim(41, scaled_data.shape[1])\n",
    "f1.set_xlabel(\"Number of Features\")\n",
    "f1.set_ylim(0.4, 0.8)\n",
    "f1.set_ylabel(\"Classification F1 Score\")\n",
    "plt.show()\n",
    "print(\"here\")\n",
    "#score = svc.score(data, target)\n",
    "##print(svc)\n",
    "#ranking = rfe.feature_importances_\n",
    "#print(\"no of feat\",ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UUID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UUID\n",
       "0    48\n",
       "1    48\n",
       "2    48\n",
       "3    48\n",
       "4    48"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aLN1</th>\n",
       "      <th>a.2</th>\n",
       "      <th>aLN3</th>\n",
       "      <th>at4</th>\n",
       "      <th>ai5</th>\n",
       "      <th>ae6</th>\n",
       "      <th>aLN7</th>\n",
       "      <th>a58</th>\n",
       "      <th>aLN9</th>\n",
       "      <th>aSH10</th>\n",
       "      <th>...</th>\n",
       "      <th>du2o12</th>\n",
       "      <th>du2a13</th>\n",
       "      <th>du2n14</th>\n",
       "      <th>du2n15</th>\n",
       "      <th>avgdu</th>\n",
       "      <th>avgud</th>\n",
       "      <th>avgdd</th>\n",
       "      <th>avguu</th>\n",
       "      <th>avdu2</th>\n",
       "      <th>avga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2382</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>973</td>\n",
       "      <td>37.875</td>\n",
       "      <td>24.466667</td>\n",
       "      <td>56.800000</td>\n",
       "      <td>55.866667</td>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.004412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>3015</td>\n",
       "      <td>1081</td>\n",
       "      <td>37.625</td>\n",
       "      <td>31.933333</td>\n",
       "      <td>64.066667</td>\n",
       "      <td>63.266667</td>\n",
       "      <td>95.400000</td>\n",
       "      <td>0.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3015</td>\n",
       "      <td>2361</td>\n",
       "      <td>1918</td>\n",
       "      <td>884</td>\n",
       "      <td>64.125</td>\n",
       "      <td>453.733333</td>\n",
       "      <td>515.933333</td>\n",
       "      <td>513.133333</td>\n",
       "      <td>575.333333</td>\n",
       "      <td>0.008333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2361</td>\n",
       "      <td>1918</td>\n",
       "      <td>1438</td>\n",
       "      <td>827</td>\n",
       "      <td>63.250</td>\n",
       "      <td>347.733333</td>\n",
       "      <td>407.733333</td>\n",
       "      <td>406.400000</td>\n",
       "      <td>466.400000</td>\n",
       "      <td>0.008211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2382</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>973</td>\n",
       "      <td>69.375</td>\n",
       "      <td>-9.133333</td>\n",
       "      <td>56.800000</td>\n",
       "      <td>55.866667</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>0.009804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aLN1       a.2      aLN3       at4       ai5       ae6      aLN7  a58  \\\n",
       "0  0.017647  0.015686  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "1  0.015686  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "2  0.017647  0.015686  0.015686  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "3  0.015686  0.015686  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "4  0.017647  0.015686  0.015686  0.019608  0.015686  0.017647  0.017647  0.0   \n",
       "\n",
       "   aLN9  aSH10    ...     du2o12     du2a13     du2n14  du2n15   avgdu  \\\n",
       "0   0.0    0.0    ...       2382       2302  670740857     973  37.875   \n",
       "1   0.0    0.0    ...       2302  670740857       3015    1081  37.625   \n",
       "2   0.0    0.0    ...       3015       2361       1918     884  64.125   \n",
       "3   0.0    0.0    ...       2361       1918       1438     827  63.250   \n",
       "4   0.0    0.0    ...       2382       2302  670740857     973  69.375   \n",
       "\n",
       "        avgud       avgdd       avguu       avdu2      avga  \n",
       "0   24.466667   56.800000   55.866667   88.200000  0.004412  \n",
       "1   31.933333   64.066667   63.266667   95.400000  0.004167  \n",
       "2  453.733333  515.933333  513.133333  575.333333  0.008333  \n",
       "3  347.733333  407.733333  406.400000  466.400000  0.008211  \n",
       "4   -9.133333   56.800000   55.866667  121.800000  0.009804  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2310 entries, 0 to 2309\n",
      "Columns: 130 entries, aLN1 to avga\n",
      "dtypes: float64(54), int64(76)\n",
      "memory usage: 2.3 MB\n",
      "initial data info None\n",
      "data is (2310, 130)\n",
      "(2310, 130)\n",
      "(2310,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S\\Anaconda3\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py:106: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.67      0.67         6\n",
      "          1       1.00      0.50      0.67         6\n",
      "          2       0.83      0.83      0.83         6\n",
      "          3       1.00      0.50      0.67         6\n",
      "          4       0.38      0.50      0.43         6\n",
      "          5       1.00      0.83      0.91         6\n",
      "          6       1.00      0.67      0.80         6\n",
      "          7       1.00      1.00      1.00         6\n",
      "          8       0.33      0.67      0.44         6\n",
      "          9       0.56      0.83      0.67         6\n",
      "         10       0.40      0.33      0.36         6\n",
      "         11       0.60      1.00      0.75         6\n",
      "         12       0.67      1.00      0.80         6\n",
      "         13       0.86      1.00      0.92         6\n",
      "         14       0.57      0.67      0.62         6\n",
      "         15       0.44      0.67      0.53         6\n",
      "         16       1.00      0.67      0.80         6\n",
      "         17       0.71      0.83      0.77         6\n",
      "         18       0.17      0.50      0.25         6\n",
      "         19       0.56      0.83      0.67         6\n",
      "         20       0.83      0.83      0.83         6\n",
      "         21       0.57      0.67      0.62         6\n",
      "         22       0.75      1.00      0.86         6\n",
      "         23       0.60      0.50      0.55         6\n",
      "         24       1.00      0.33      0.50         6\n",
      "         25       0.50      0.50      0.50         6\n",
      "         26       0.71      0.83      0.77         6\n",
      "         27       0.00      0.00      0.00         6\n",
      "         28       0.71      0.83      0.77         6\n",
      "         29       1.00      0.50      0.67         6\n",
      "         30       0.71      0.83      0.77         6\n",
      "         31       0.67      1.00      0.80         6\n",
      "         32       0.67      1.00      0.80         6\n",
      "         33       0.83      0.83      0.83         6\n",
      "         34       0.60      1.00      0.75         6\n",
      "         35       0.71      0.83      0.77         6\n",
      "         36       1.00      0.50      0.67         6\n",
      "         37       1.00      0.67      0.80         6\n",
      "         38       0.62      0.83      0.71         6\n",
      "         39       0.40      0.33      0.36         6\n",
      "         40       0.00      0.00      0.00         6\n",
      "         41       0.50      0.67      0.57         6\n",
      "         42       0.80      0.67      0.73         6\n",
      "         43       0.57      0.67      0.62         6\n",
      "         44       0.75      0.50      0.60         6\n",
      "         45       0.80      0.67      0.73         6\n",
      "         46       0.50      0.50      0.50         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       1.00      0.33      0.50         6\n",
      "         49       1.00      0.83      0.91         6\n",
      "         50       0.43      0.50      0.46         6\n",
      "         51       0.00      0.00      0.00         6\n",
      "         52       0.60      0.50      0.55         6\n",
      "         53       0.62      0.83      0.71         6\n",
      "         54       0.86      1.00      0.92         6\n",
      "         55       1.00      0.67      0.80         6\n",
      "         56       1.00      0.33      0.50         6\n",
      "         57       1.00      0.50      0.67         6\n",
      "         58       0.83      0.83      0.83         6\n",
      "         59       1.00      0.67      0.80         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       0.75      1.00      0.86         6\n",
      "         62       0.50      0.50      0.50         6\n",
      "         63       0.71      0.83      0.77         6\n",
      "         64       0.75      0.50      0.60         6\n",
      "         65       1.00      0.33      0.50         6\n",
      "         66       0.00      0.00      0.00         6\n",
      "         67       1.00      0.50      0.67         6\n",
      "         68       1.00      1.00      1.00         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       0.67      0.67      0.67         6\n",
      "         71       1.00      1.00      1.00         6\n",
      "         72       0.50      0.17      0.25         6\n",
      "         73       0.43      0.50      0.46         6\n",
      "         74       0.83      0.83      0.83         6\n",
      "         75       0.17      0.33      0.22         6\n",
      "         76       0.50      0.67      0.57         6\n",
      "\n",
      "avg / total       0.70      0.66      0.65       462\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.50      0.67         6\n",
      "          1       1.00      0.83      0.91         6\n",
      "          2       0.83      0.83      0.83         6\n",
      "          3       0.86      1.00      0.92         6\n",
      "          4       0.67      1.00      0.80         6\n",
      "          5       1.00      0.83      0.91         6\n",
      "          6       1.00      0.67      0.80         6\n",
      "          7       1.00      0.83      0.91         6\n",
      "          8       0.33      0.50      0.40         6\n",
      "          9       0.44      0.67      0.53         6\n",
      "         10       0.83      0.83      0.83         6\n",
      "         11       1.00      1.00      1.00         6\n",
      "         12       0.67      1.00      0.80         6\n",
      "         13       1.00      1.00      1.00         6\n",
      "         14       1.00      0.83      0.91         6\n",
      "         15       0.75      1.00      0.86         6\n",
      "         16       1.00      1.00      1.00         6\n",
      "         17       0.83      0.83      0.83         6\n",
      "         18       0.50      0.50      0.50         6\n",
      "         19       0.67      1.00      0.80         6\n",
      "         20       0.83      0.83      0.83         6\n",
      "         21       0.50      0.67      0.57         6\n",
      "         22       0.75      1.00      0.86         6\n",
      "         23       0.83      0.83      0.83         6\n",
      "         24       0.50      0.33      0.40         6\n",
      "         25       0.50      1.00      0.67         6\n",
      "         26       0.86      1.00      0.92         6\n",
      "         27       0.30      0.50      0.37         6\n",
      "         28       1.00      0.83      0.91         6\n",
      "         29       0.57      0.67      0.62         6\n",
      "         30       0.67      0.67      0.67         6\n",
      "         31       0.67      0.67      0.67         6\n",
      "         32       0.62      0.83      0.71         6\n",
      "         33       0.67      1.00      0.80         6\n",
      "         34       0.57      0.67      0.62         6\n",
      "         35       0.62      0.83      0.71         6\n",
      "         36       1.00      0.33      0.50         6\n",
      "         37       1.00      0.50      0.67         6\n",
      "         38       0.86      1.00      0.92         6\n",
      "         39       0.50      0.67      0.57         6\n",
      "         40       1.00      0.33      0.50         6\n",
      "         41       0.67      0.33      0.44         6\n",
      "         42       1.00      0.33      0.50         6\n",
      "         43       0.55      1.00      0.71         6\n",
      "         44       0.75      0.50      0.60         6\n",
      "         45       1.00      1.00      1.00         6\n",
      "         46       0.50      0.33      0.40         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       0.71      0.83      0.77         6\n",
      "         49       1.00      0.83      0.91         6\n",
      "         50       0.50      0.83      0.62         6\n",
      "         51       0.50      0.33      0.40         6\n",
      "         52       0.33      0.33      0.33         6\n",
      "         53       0.55      1.00      0.71         6\n",
      "         54       0.75      1.00      0.86         6\n",
      "         55       1.00      1.00      1.00         6\n",
      "         56       1.00      0.33      0.50         6\n",
      "         57       1.00      1.00      1.00         6\n",
      "         58       1.00      0.50      0.67         6\n",
      "         59       1.00      1.00      1.00         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       0.75      0.50      0.60         6\n",
      "         63       0.71      0.83      0.77         6\n",
      "         64       0.75      0.50      0.60         6\n",
      "         65       0.50      0.17      0.25         6\n",
      "         66       0.71      0.83      0.77         6\n",
      "         67       0.67      0.33      0.44         6\n",
      "         68       1.00      1.00      1.00         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       0.67      0.67      0.67         6\n",
      "         71       1.00      1.00      1.00         6\n",
      "         72       0.75      0.50      0.60         6\n",
      "         73       0.60      0.50      0.55         6\n",
      "         74       1.00      1.00      1.00         6\n",
      "         75       1.00      0.17      0.29         6\n",
      "         76       0.80      0.67      0.73         6\n",
      "\n",
      "avg / total       0.78      0.74      0.73       462\n",
      "\n",
      "each loop acc 0.660173160173\n",
      "each loop rbf acc 0.74025974026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\S\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.83      0.83         6\n",
      "          1       0.67      0.67      0.67         6\n",
      "          2       0.67      1.00      0.80         6\n",
      "          3       0.83      0.83      0.83         6\n",
      "          4       0.67      0.67      0.67         6\n",
      "          5       1.00      0.83      0.91         6\n",
      "          6       1.00      1.00      1.00         6\n",
      "          7       1.00      1.00      1.00         6\n",
      "          8       0.18      0.33      0.24         6\n",
      "          9       0.67      0.33      0.44         6\n",
      "         10       0.67      0.33      0.44         6\n",
      "         11       0.50      0.33      0.40         6\n",
      "         12       0.75      1.00      0.86         6\n",
      "         13       1.00      0.83      0.91         6\n",
      "         14       0.56      0.83      0.67         6\n",
      "         15       0.60      0.50      0.55         6\n",
      "         16       1.00      0.83      0.91         6\n",
      "         17       0.83      0.83      0.83         6\n",
      "         18       0.23      0.50      0.32         6\n",
      "         19       0.71      0.83      0.77         6\n",
      "         20       0.71      0.83      0.77         6\n",
      "         21       1.00      0.33      0.50         6\n",
      "         22       0.67      1.00      0.80         6\n",
      "         23       0.14      0.17      0.15         6\n",
      "         24       1.00      0.67      0.80         6\n",
      "         25       0.50      0.17      0.25         6\n",
      "         26       0.71      0.83      0.77         6\n",
      "         27       0.25      0.17      0.20         6\n",
      "         28       0.71      0.83      0.77         6\n",
      "         29       1.00      0.50      0.67         6\n",
      "         30       0.83      0.83      0.83         6\n",
      "         31       0.67      1.00      0.80         6\n",
      "         32       1.00      0.83      0.91         6\n",
      "         33       0.71      0.83      0.77         6\n",
      "         34       0.55      1.00      0.71         6\n",
      "         35       0.83      0.83      0.83         6\n",
      "         36       0.50      0.67      0.57         6\n",
      "         37       0.50      0.67      0.57         6\n",
      "         38       0.83      0.83      0.83         6\n",
      "         39       0.50      0.83      0.62         6\n",
      "         40       0.67      0.67      0.67         6\n",
      "         41       0.50      0.67      0.57         6\n",
      "         42       1.00      0.67      0.80         6\n",
      "         43       1.00      1.00      1.00         6\n",
      "         44       1.00      0.67      0.80         6\n",
      "         45       0.60      0.50      0.55         6\n",
      "         46       0.50      0.83      0.62         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       1.00      0.50      0.67         6\n",
      "         49       1.00      0.67      0.80         6\n",
      "         50       0.33      0.50      0.40         6\n",
      "         51       0.00      0.00      0.00         6\n",
      "         52       0.67      0.33      0.44         6\n",
      "         53       0.67      1.00      0.80         6\n",
      "         54       0.86      1.00      0.92         6\n",
      "         55       1.00      0.50      0.67         6\n",
      "         56       1.00      0.17      0.29         6\n",
      "         57       0.83      0.83      0.83         6\n",
      "         58       0.83      0.83      0.83         6\n",
      "         59       1.00      0.67      0.80         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       0.75      1.00      0.86         6\n",
      "         62       0.67      0.67      0.67         6\n",
      "         63       1.00      1.00      1.00         6\n",
      "         64       0.83      0.83      0.83         6\n",
      "         65       0.50      0.17      0.25         6\n",
      "         66       0.50      0.33      0.40         6\n",
      "         67       0.33      0.33      0.33         6\n",
      "         68       1.00      1.00      1.00         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       0.86      1.00      0.92         6\n",
      "         71       1.00      0.83      0.91         6\n",
      "         72       0.62      0.83      0.71         6\n",
      "         73       0.75      0.50      0.60         6\n",
      "         74       0.83      0.83      0.83         6\n",
      "         75       0.33      0.50      0.40         6\n",
      "         76       0.75      0.50      0.60         6\n",
      "\n",
      "avg / total       0.73      0.69      0.69       462\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91         6\n",
      "          1       0.83      0.83      0.83         6\n",
      "          2       0.75      1.00      0.86         6\n",
      "          3       1.00      0.83      0.91         6\n",
      "          4       0.60      1.00      0.75         6\n",
      "          5       1.00      0.83      0.91         6\n",
      "          6       1.00      1.00      1.00         6\n",
      "          7       1.00      1.00      1.00         6\n",
      "          8       0.45      0.83      0.59         6\n",
      "          9       0.75      1.00      0.86         6\n",
      "         10       0.57      0.67      0.62         6\n",
      "         11       1.00      0.83      0.91         6\n",
      "         12       0.86      1.00      0.92         6\n",
      "         13       1.00      0.83      0.91         6\n",
      "         14       1.00      1.00      1.00         6\n",
      "         15       0.62      0.83      0.71         6\n",
      "         16       0.86      1.00      0.92         6\n",
      "         17       1.00      1.00      1.00         6\n",
      "         18       0.33      0.33      0.33         6\n",
      "         19       0.71      0.83      0.77         6\n",
      "         20       0.80      0.67      0.73         6\n",
      "         21       0.67      0.33      0.44         6\n",
      "         22       0.86      1.00      0.92         6\n",
      "         23       0.33      0.33      0.33         6\n",
      "         24       0.57      0.67      0.62         6\n",
      "         25       0.33      0.17      0.22         6\n",
      "         26       0.80      0.67      0.73         6\n",
      "         27       0.50      0.50      0.50         6\n",
      "         28       0.83      0.83      0.83         6\n",
      "         29       0.75      0.50      0.60         6\n",
      "         30       0.71      0.83      0.77         6\n",
      "         31       0.62      0.83      0.71         6\n",
      "         32       0.86      1.00      0.92         6\n",
      "         33       0.57      0.67      0.62         6\n",
      "         34       0.50      0.67      0.57         6\n",
      "         35       0.83      0.83      0.83         6\n",
      "         36       0.67      0.67      0.67         6\n",
      "         37       0.60      0.50      0.55         6\n",
      "         38       1.00      1.00      1.00         6\n",
      "         39       0.80      0.67      0.73         6\n",
      "         40       1.00      0.33      0.50         6\n",
      "         41       0.40      0.67      0.50         6\n",
      "         42       1.00      0.50      0.67         6\n",
      "         43       0.83      0.83      0.83         6\n",
      "         44       0.75      0.50      0.60         6\n",
      "         45       0.50      0.67      0.57         6\n",
      "         46       0.67      0.67      0.67         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       0.50      0.50      0.50         6\n",
      "         49       1.00      0.50      0.67         6\n",
      "         50       0.50      0.50      0.50         6\n",
      "         51       0.33      0.17      0.22         6\n",
      "         52       0.40      0.33      0.36         6\n",
      "         53       0.60      1.00      0.75         6\n",
      "         54       0.86      1.00      0.92         6\n",
      "         55       1.00      1.00      1.00         6\n",
      "         56       1.00      0.50      0.67         6\n",
      "         57       0.86      1.00      0.92         6\n",
      "         58       0.67      0.67      0.67         6\n",
      "         59       1.00      1.00      1.00         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       0.80      0.67      0.73         6\n",
      "         63       1.00      1.00      1.00         6\n",
      "         64       0.75      1.00      0.86         6\n",
      "         65       0.33      0.17      0.22         6\n",
      "         66       0.57      0.67      0.62         6\n",
      "         67       0.50      0.50      0.50         6\n",
      "         68       1.00      1.00      1.00         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       1.00      0.83      0.91         6\n",
      "         71       1.00      0.83      0.91         6\n",
      "         72       0.83      0.83      0.83         6\n",
      "         73       0.83      0.83      0.83         6\n",
      "         74       1.00      1.00      1.00         6\n",
      "         75       0.60      0.50      0.55         6\n",
      "         76       0.67      0.67      0.67         6\n",
      "\n",
      "avg / total       0.76      0.75      0.74       462\n",
      "\n",
      "each loop acc 0.694805194805\n",
      "each loop rbf acc 0.748917748918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.50      0.55         6\n",
      "          1       0.67      1.00      0.80         6\n",
      "          2       0.44      0.67      0.53         6\n",
      "          3       1.00      0.50      0.67         6\n",
      "          4       0.33      0.33      0.33         6\n",
      "          5       1.00      1.00      1.00         6\n",
      "          6       1.00      1.00      1.00         6\n",
      "          7       0.83      0.83      0.83         6\n",
      "          8       0.20      0.33      0.25         6\n",
      "          9       0.33      0.50      0.40         6\n",
      "         10       0.17      0.17      0.17         6\n",
      "         11       0.60      1.00      0.75         6\n",
      "         12       0.42      0.83      0.56         6\n",
      "         13       1.00      0.83      0.91         6\n",
      "         14       0.75      1.00      0.86         6\n",
      "         15       0.50      0.67      0.57         6\n",
      "         16       1.00      0.67      0.80         6\n",
      "         17       0.83      0.83      0.83         6\n",
      "         18       0.15      0.67      0.25         6\n",
      "         19       1.00      0.83      0.91         6\n",
      "         20       0.80      0.67      0.73         6\n",
      "         21       1.00      0.50      0.67         6\n",
      "         22       0.86      1.00      0.92         6\n",
      "         23       0.60      0.50      0.55         6\n",
      "         24       0.00      0.00      0.00         6\n",
      "         25       0.00      0.00      0.00         6\n",
      "         26       0.80      0.67      0.73         6\n",
      "         27       0.00      0.00      0.00         6\n",
      "         28       0.56      0.83      0.67         6\n",
      "         29       0.75      0.50      0.60         6\n",
      "         30       0.60      0.50      0.55         6\n",
      "         31       0.56      0.83      0.67         6\n",
      "         32       1.00      0.67      0.80         6\n",
      "         33       0.67      0.33      0.44         6\n",
      "         34       0.57      0.67      0.62         6\n",
      "         35       0.43      0.50      0.46         6\n",
      "         36       0.50      0.33      0.40         6\n",
      "         37       1.00      0.33      0.50         6\n",
      "         38       0.80      0.67      0.73         6\n",
      "         39       0.83      0.83      0.83         6\n",
      "         40       0.57      0.67      0.62         6\n",
      "         41       0.38      1.00      0.55         6\n",
      "         42       0.50      0.50      0.50         6\n",
      "         43       0.62      0.83      0.71         6\n",
      "         44       0.57      0.67      0.62         6\n",
      "         45       0.00      0.00      0.00         6\n",
      "         46       0.33      0.33      0.33         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       1.00      0.33      0.50         6\n",
      "         49       1.00      0.83      0.91         6\n",
      "         50       0.30      0.50      0.37         6\n",
      "         51       0.00      0.00      0.00         6\n",
      "         52       0.00      0.00      0.00         6\n",
      "         53       0.50      0.50      0.50         6\n",
      "         54       0.83      0.83      0.83         6\n",
      "         55       0.67      0.67      0.67         6\n",
      "         56       1.00      0.50      0.67         6\n",
      "         57       1.00      0.33      0.50         6\n",
      "         58       0.56      0.83      0.67         6\n",
      "         59       1.00      0.50      0.67         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       0.67      1.00      0.80         6\n",
      "         62       0.40      0.33      0.36         6\n",
      "         63       0.80      0.67      0.73         6\n",
      "         64       0.67      0.67      0.67         6\n",
      "         65       0.50      0.17      0.25         6\n",
      "         66       0.33      0.17      0.22         6\n",
      "         67       1.00      0.33      0.50         6\n",
      "         68       1.00      1.00      1.00         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       0.67      0.67      0.67         6\n",
      "         71       1.00      1.00      1.00         6\n",
      "         72       0.67      0.33      0.44         6\n",
      "         73       0.50      0.50      0.50         6\n",
      "         74       0.71      0.83      0.77         6\n",
      "         75       0.18      0.33      0.24         6\n",
      "         76       0.57      0.67      0.62         6\n",
      "\n",
      "avg / total       0.63      0.60      0.59       462\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.83      0.71         6\n",
      "          1       1.00      1.00      1.00         6\n",
      "          2       0.67      1.00      0.80         6\n",
      "          3       1.00      0.67      0.80         6\n",
      "          4       0.60      0.50      0.55         6\n",
      "          5       1.00      1.00      1.00         6\n",
      "          6       1.00      1.00      1.00         6\n",
      "          7       0.83      0.83      0.83         6\n",
      "          8       0.29      0.67      0.40         6\n",
      "          9       0.38      0.50      0.43         6\n",
      "         10       0.43      0.50      0.46         6\n",
      "         11       0.86      1.00      0.92         6\n",
      "         12       0.50      0.83      0.62         6\n",
      "         13       1.00      0.83      0.91         6\n",
      "         14       1.00      1.00      1.00         6\n",
      "         15       0.60      0.50      0.55         6\n",
      "         16       1.00      1.00      1.00         6\n",
      "         17       0.83      0.83      0.83         6\n",
      "         18       0.60      0.50      0.55         6\n",
      "         19       1.00      1.00      1.00         6\n",
      "         20       1.00      0.83      0.91         6\n",
      "         21       1.00      0.67      0.80         6\n",
      "         22       0.86      1.00      0.92         6\n",
      "         23       0.60      0.50      0.55         6\n",
      "         24       0.20      0.17      0.18         6\n",
      "         25       0.67      0.67      0.67         6\n",
      "         26       1.00      0.83      0.91         6\n",
      "         27       0.50      0.33      0.40         6\n",
      "         28       0.83      0.83      0.83         6\n",
      "         29       0.43      0.50      0.46         6\n",
      "         30       0.80      0.67      0.73         6\n",
      "         31       0.20      0.17      0.18         6\n",
      "         32       0.83      0.83      0.83         6\n",
      "         33       0.67      0.67      0.67         6\n",
      "         34       0.40      0.33      0.36         6\n",
      "         35       0.43      0.50      0.46         6\n",
      "         36       0.75      0.50      0.60         6\n",
      "         37       0.57      0.67      0.62         6\n",
      "         38       1.00      1.00      1.00         6\n",
      "         39       0.71      0.83      0.77         6\n",
      "         40       0.57      0.67      0.62         6\n",
      "         41       1.00      0.83      0.91         6\n",
      "         42       0.43      0.50      0.46         6\n",
      "         43       0.83      0.83      0.83         6\n",
      "         44       0.80      0.67      0.73         6\n",
      "         45       0.50      0.33      0.40         6\n",
      "         46       0.33      0.33      0.33         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       0.50      0.67      0.57         6\n",
      "         49       1.00      1.00      1.00         6\n",
      "         50       0.29      0.33      0.31         6\n",
      "         51       0.20      0.17      0.18         6\n",
      "         52       0.11      0.17      0.13         6\n",
      "         53       0.40      0.33      0.36         6\n",
      "         54       0.83      0.83      0.83         6\n",
      "         55       0.86      1.00      0.92         6\n",
      "         56       1.00      0.33      0.50         6\n",
      "         57       1.00      0.83      0.91         6\n",
      "         58       0.67      0.67      0.67         6\n",
      "         59       1.00      0.67      0.80         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       0.75      1.00      0.86         6\n",
      "         62       0.40      0.33      0.36         6\n",
      "         63       1.00      1.00      1.00         6\n",
      "         64       0.83      0.83      0.83         6\n",
      "         65       0.25      0.17      0.20         6\n",
      "         66       0.50      0.50      0.50         6\n",
      "         67       0.50      0.50      0.50         6\n",
      "         68       1.00      1.00      1.00         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       0.67      0.67      0.67         6\n",
      "         71       1.00      1.00      1.00         6\n",
      "         72       0.62      0.83      0.71         6\n",
      "         73       0.71      0.83      0.77         6\n",
      "         74       1.00      1.00      1.00         6\n",
      "         75       0.67      0.33      0.44         6\n",
      "         76       0.75      0.50      0.60         6\n",
      "\n",
      "avg / total       0.71      0.69      0.69       462\n",
      "\n",
      "each loop acc 0.597402597403\n",
      "each loop rbf acc 0.690476190476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.67      0.57         6\n",
      "          1       0.62      0.83      0.71         6\n",
      "          2       0.75      1.00      0.86         6\n",
      "          3       0.83      0.83      0.83         6\n",
      "          4       0.67      0.67      0.67         6\n",
      "          5       1.00      1.00      1.00         6\n",
      "          6       0.86      1.00      0.92         6\n",
      "          7       1.00      0.83      0.91         6\n",
      "          8       0.23      0.50      0.32         6\n",
      "          9       0.50      0.50      0.50         6\n",
      "         10       0.29      0.33      0.31         6\n",
      "         11       1.00      0.83      0.91         6\n",
      "         12       0.67      0.67      0.67         6\n",
      "         13       0.62      0.83      0.71         6\n",
      "         14       0.55      1.00      0.71         6\n",
      "         15       0.80      0.67      0.73         6\n",
      "         16       1.00      0.67      0.80         6\n",
      "         17       0.80      0.67      0.73         6\n",
      "         18       0.22      0.33      0.27         6\n",
      "         19       0.60      0.50      0.55         6\n",
      "         20       1.00      0.83      0.91         6\n",
      "         21       0.62      0.83      0.71         6\n",
      "         22       0.80      0.67      0.73         6\n",
      "         23       0.50      0.83      0.62         6\n",
      "         24       0.50      0.17      0.25         6\n",
      "         25       0.38      0.50      0.43         6\n",
      "         26       0.67      0.67      0.67         6\n",
      "         27       0.25      0.17      0.20         6\n",
      "         28       0.80      0.67      0.73         6\n",
      "         29       0.62      0.83      0.71         6\n",
      "         30       0.57      0.67      0.62         6\n",
      "         31       0.80      0.67      0.73         6\n",
      "         32       0.80      0.67      0.73         6\n",
      "         33       1.00      0.83      0.91         6\n",
      "         34       0.83      0.83      0.83         6\n",
      "         35       0.57      0.67      0.62         6\n",
      "         36       0.60      0.50      0.55         6\n",
      "         37       1.00      0.83      0.91         6\n",
      "         38       0.71      0.83      0.77         6\n",
      "         39       0.33      0.17      0.22         6\n",
      "         40       0.67      0.67      0.67         6\n",
      "         41       0.50      0.67      0.57         6\n",
      "         42       0.71      0.83      0.77         6\n",
      "         43       1.00      0.33      0.50         6\n",
      "         44       0.40      0.67      0.50         6\n",
      "         45       0.67      0.67      0.67         6\n",
      "         46       0.33      0.33      0.33         6\n",
      "         47       1.00      0.50      0.67         6\n",
      "         48       1.00      0.50      0.67         6\n",
      "         49       1.00      1.00      1.00         6\n",
      "         50       0.00      0.00      0.00         6\n",
      "         51       0.67      0.33      0.44         6\n",
      "         52       0.20      0.17      0.18         6\n",
      "         53       0.83      0.83      0.83         6\n",
      "         54       0.86      1.00      0.92         6\n",
      "         55       1.00      0.83      0.91         6\n",
      "         56       0.50      0.50      0.50         6\n",
      "         57       0.86      1.00      0.92         6\n",
      "         58       0.75      1.00      0.86         6\n",
      "         59       1.00      1.00      1.00         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       0.57      0.67      0.62         6\n",
      "         63       1.00      0.83      0.91         6\n",
      "         64       0.86      1.00      0.92         6\n",
      "         65       0.33      0.33      0.33         6\n",
      "         66       0.25      0.17      0.20         6\n",
      "         67       0.50      0.17      0.25         6\n",
      "         68       1.00      0.67      0.80         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       0.67      0.33      0.44         6\n",
      "         71       1.00      0.83      0.91         6\n",
      "         72       0.75      0.50      0.60         6\n",
      "         73       0.75      0.50      0.60         6\n",
      "         74       0.75      0.50      0.60         6\n",
      "         75       0.33      0.67      0.44         6\n",
      "         76       0.67      1.00      0.80         6\n",
      "\n",
      "avg / total       0.69      0.66      0.66       462\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      1.00      0.86         6\n",
      "          1       0.86      1.00      0.92         6\n",
      "          2       0.67      0.67      0.67         6\n",
      "          3       0.67      1.00      0.80         6\n",
      "          4       0.62      0.83      0.71         6\n",
      "          5       1.00      1.00      1.00         6\n",
      "          6       1.00      1.00      1.00         6\n",
      "          7       0.83      0.83      0.83         6\n",
      "          8       0.57      0.67      0.62         6\n",
      "          9       0.67      0.67      0.67         6\n",
      "         10       0.83      0.83      0.83         6\n",
      "         11       1.00      0.83      0.91         6\n",
      "         12       0.83      0.83      0.83         6\n",
      "         13       0.75      1.00      0.86         6\n",
      "         14       0.86      1.00      0.92         6\n",
      "         15       0.67      0.67      0.67         6\n",
      "         16       0.83      0.83      0.83         6\n",
      "         17       1.00      1.00      1.00         6\n",
      "         18       0.20      0.17      0.18         6\n",
      "         19       0.86      1.00      0.92         6\n",
      "         20       0.80      0.67      0.73         6\n",
      "         21       0.67      1.00      0.80         6\n",
      "         22       0.83      0.83      0.83         6\n",
      "         23       0.55      1.00      0.71         6\n",
      "         24       0.60      0.50      0.55         6\n",
      "         25       0.50      0.67      0.57         6\n",
      "         26       0.71      0.83      0.77         6\n",
      "         27       0.67      0.67      0.67         6\n",
      "         28       1.00      0.83      0.91         6\n",
      "         29       0.50      0.50      0.50         6\n",
      "         30       0.83      0.83      0.83         6\n",
      "         31       0.60      0.50      0.55         6\n",
      "         32       0.83      0.83      0.83         6\n",
      "         33       0.67      0.67      0.67         6\n",
      "         34       0.75      0.50      0.60         6\n",
      "         35       0.71      0.83      0.77         6\n",
      "         36       0.75      0.50      0.60         6\n",
      "         37       0.62      0.83      0.71         6\n",
      "         38       0.83      0.83      0.83         6\n",
      "         39       0.75      1.00      0.86         6\n",
      "         40       0.60      0.50      0.55         6\n",
      "         41       0.40      0.33      0.36         6\n",
      "         42       0.56      0.83      0.67         6\n",
      "         43       1.00      0.67      0.80         6\n",
      "         44       0.44      0.67      0.53         6\n",
      "         45       0.86      1.00      0.92         6\n",
      "         46       0.50      0.17      0.25         6\n",
      "         47       1.00      0.50      0.67         6\n",
      "         48       1.00      1.00      1.00         6\n",
      "         49       1.00      1.00      1.00         6\n",
      "         50       1.00      0.67      0.80         6\n",
      "         51       0.67      0.33      0.44         6\n",
      "         52       0.50      0.33      0.40         6\n",
      "         53       0.60      0.50      0.55         6\n",
      "         54       1.00      1.00      1.00         6\n",
      "         55       1.00      0.83      0.91         6\n",
      "         56       0.67      0.33      0.44         6\n",
      "         57       0.86      1.00      0.92         6\n",
      "         58       0.67      0.67      0.67         6\n",
      "         59       1.00      0.83      0.91         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       1.00      0.83      0.91         6\n",
      "         62       0.67      1.00      0.80         6\n",
      "         63       1.00      0.83      0.91         6\n",
      "         64       0.71      0.83      0.77         6\n",
      "         65       1.00      0.67      0.80         6\n",
      "         66       0.75      1.00      0.86         6\n",
      "         67       0.83      0.83      0.83         6\n",
      "         68       1.00      0.67      0.80         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       0.75      0.50      0.60         6\n",
      "         71       1.00      0.83      0.91         6\n",
      "         72       0.80      0.67      0.73         6\n",
      "         73       0.71      0.83      0.77         6\n",
      "         74       1.00      0.67      0.80         6\n",
      "         75       0.62      0.83      0.71         6\n",
      "         76       0.83      0.83      0.83         6\n",
      "\n",
      "avg / total       0.77      0.76      0.76       462\n",
      "\n",
      "each loop acc 0.664502164502\n",
      "each loop rbf acc 0.761904761905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.33      0.50         6\n",
      "          1       0.83      0.83      0.83         6\n",
      "          2       0.67      0.67      0.67         6\n",
      "          3       0.57      0.67      0.62         6\n",
      "          4       0.56      0.83      0.67         6\n",
      "          5       1.00      1.00      1.00         6\n",
      "          6       1.00      1.00      1.00         6\n",
      "          7       0.86      1.00      0.92         6\n",
      "          8       0.18      0.33      0.24         6\n",
      "          9       0.45      0.83      0.59         6\n",
      "         10       0.67      0.33      0.44         6\n",
      "         11       0.67      1.00      0.80         6\n",
      "         12       0.55      1.00      0.71         6\n",
      "         13       1.00      0.83      0.91         6\n",
      "         14       0.83      0.83      0.83         6\n",
      "         15       0.55      1.00      0.71         6\n",
      "         16       1.00      1.00      1.00         6\n",
      "         17       0.71      0.83      0.77         6\n",
      "         18       0.28      0.83      0.42         6\n",
      "         19       0.83      0.83      0.83         6\n",
      "         20       0.86      1.00      0.92         6\n",
      "         21       0.80      0.67      0.73         6\n",
      "         22       0.83      0.83      0.83         6\n",
      "         23       1.00      0.33      0.50         6\n",
      "         24       1.00      0.33      0.50         6\n",
      "         25       0.50      0.67      0.57         6\n",
      "         26       0.86      1.00      0.92         6\n",
      "         27       0.17      0.17      0.17         6\n",
      "         28       0.50      0.83      0.62         6\n",
      "         29       0.67      0.67      0.67         6\n",
      "         30       0.38      0.50      0.43         6\n",
      "         31       0.60      0.50      0.55         6\n",
      "         32       0.71      0.83      0.77         6\n",
      "         33       0.71      0.83      0.77         6\n",
      "         34       0.67      0.67      0.67         6\n",
      "         35       0.67      0.67      0.67         6\n",
      "         36       0.50      0.17      0.25         6\n",
      "         37       1.00      0.50      0.67         6\n",
      "         38       1.00      0.50      0.67         6\n",
      "         39       0.50      0.67      0.57         6\n",
      "         40       0.67      0.33      0.44         6\n",
      "         41       0.29      0.33      0.31         6\n",
      "         42       0.75      0.50      0.60         6\n",
      "         43       0.44      0.67      0.53         6\n",
      "         44       0.80      0.67      0.73         6\n",
      "         45       0.67      0.33      0.44         6\n",
      "         46       0.40      0.33      0.36         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       1.00      0.67      0.80         6\n",
      "         49       0.86      1.00      0.92         6\n",
      "         50       0.38      0.50      0.43         6\n",
      "         51       0.00      0.00      0.00         6\n",
      "         52       0.50      0.50      0.50         6\n",
      "         53       0.62      0.83      0.71         6\n",
      "         54       1.00      1.00      1.00         6\n",
      "         55       0.67      0.67      0.67         6\n",
      "         56       0.71      0.83      0.77         6\n",
      "         57       1.00      0.50      0.67         6\n",
      "         58       0.80      0.67      0.73         6\n",
      "         59       1.00      0.67      0.80         6\n",
      "         60       1.00      0.83      0.91         6\n",
      "         61       0.75      1.00      0.86         6\n",
      "         62       0.71      0.83      0.77         6\n",
      "         63       1.00      1.00      1.00         6\n",
      "         64       1.00      1.00      1.00         6\n",
      "         65       1.00      0.33      0.50         6\n",
      "         66       0.50      0.33      0.40         6\n",
      "         67       1.00      0.33      0.50         6\n",
      "         68       0.83      0.83      0.83         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       0.67      0.67      0.67         6\n",
      "         71       1.00      1.00      1.00         6\n",
      "         72       1.00      0.17      0.29         6\n",
      "         73       0.50      0.17      0.25         6\n",
      "         74       0.62      0.83      0.71         6\n",
      "         75       0.00      0.00      0.00         6\n",
      "         76       0.50      0.83      0.62         6\n",
      "\n",
      "avg / total       0.71      0.67      0.66       462\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.67      0.73         6\n",
      "          1       1.00      0.83      0.91         6\n",
      "          2       0.71      0.83      0.77         6\n",
      "          3       1.00      0.67      0.80         6\n",
      "          4       0.75      1.00      0.86         6\n",
      "          5       1.00      1.00      1.00         6\n",
      "          6       1.00      1.00      1.00         6\n",
      "          7       0.86      1.00      0.92         6\n",
      "          8       0.33      0.83      0.48         6\n",
      "          9       0.75      0.50      0.60         6\n",
      "         10       1.00      0.67      0.80         6\n",
      "         11       1.00      1.00      1.00         6\n",
      "         12       0.55      1.00      0.71         6\n",
      "         13       1.00      1.00      1.00         6\n",
      "         14       0.86      1.00      0.92         6\n",
      "         15       0.30      0.50      0.37         6\n",
      "         16       1.00      1.00      1.00         6\n",
      "         17       0.75      1.00      0.86         6\n",
      "         18       0.50      0.50      0.50         6\n",
      "         19       0.71      0.83      0.77         6\n",
      "         20       0.86      1.00      0.92         6\n",
      "         21       0.75      1.00      0.86         6\n",
      "         22       0.83      0.83      0.83         6\n",
      "         23       0.67      0.67      0.67         6\n",
      "         24       0.40      0.33      0.36         6\n",
      "         25       0.56      0.83      0.67         6\n",
      "         26       0.83      0.83      0.83         6\n",
      "         27       0.50      0.33      0.40         6\n",
      "         28       0.62      0.83      0.71         6\n",
      "         29       0.57      0.67      0.62         6\n",
      "         30       0.40      0.67      0.50         6\n",
      "         31       0.50      0.33      0.40         6\n",
      "         32       1.00      0.50      0.67         6\n",
      "         33       1.00      0.83      0.91         6\n",
      "         34       0.40      0.33      0.36         6\n",
      "         35       0.75      0.50      0.60         6\n",
      "         36       0.50      0.17      0.25         6\n",
      "         37       0.60      0.50      0.55         6\n",
      "         38       1.00      1.00      1.00         6\n",
      "         39       0.57      0.67      0.62         6\n",
      "         40       1.00      0.67      0.80         6\n",
      "         41       0.50      0.33      0.40         6\n",
      "         42       0.71      0.83      0.77         6\n",
      "         43       0.25      0.17      0.20         6\n",
      "         44       0.80      0.67      0.73         6\n",
      "         45       0.75      0.50      0.60         6\n",
      "         46       1.00      0.50      0.67         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       0.86      1.00      0.92         6\n",
      "         49       1.00      1.00      1.00         6\n",
      "         50       0.71      0.83      0.77         6\n",
      "         51       0.33      0.33      0.33         6\n",
      "         52       0.67      0.33      0.44         6\n",
      "         53       0.80      0.67      0.73         6\n",
      "         54       1.00      1.00      1.00         6\n",
      "         55       0.71      0.83      0.77         6\n",
      "         56       0.71      0.83      0.77         6\n",
      "         57       1.00      1.00      1.00         6\n",
      "         58       0.86      1.00      0.92         6\n",
      "         59       1.00      1.00      1.00         6\n",
      "         60       1.00      0.83      0.91         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       0.83      0.83      0.83         6\n",
      "         63       0.80      0.67      0.73         6\n",
      "         64       1.00      1.00      1.00         6\n",
      "         65       0.50      0.33      0.40         6\n",
      "         66       0.71      0.83      0.77         6\n",
      "         67       0.50      0.83      0.62         6\n",
      "         68       1.00      0.83      0.91         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       0.56      0.83      0.67         6\n",
      "         71       1.00      1.00      1.00         6\n",
      "         72       0.71      0.83      0.77         6\n",
      "         73       1.00      0.50      0.67         6\n",
      "         74       1.00      1.00      1.00         6\n",
      "         75       0.00      0.00      0.00         6\n",
      "         76       0.60      0.50      0.55         6\n",
      "\n",
      "avg / total       0.75      0.74      0.73       462\n",
      "\n",
      "each loop acc 0.668831168831\n",
      "each loop rbf acc 0.74025974026\n",
      "f1  0.6487112965215978\n",
      "Accuracy: 0.6571428571428571\n",
      "f1  0.7298383184594033\n",
      "Accuracy: 0.7363636363636363\n",
      "here\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgEAAADqCAYAAAA7+pYJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc4f3550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import lsanomaly\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn import utils  \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "\n",
    "# import the CSV from http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "# this will return a pandas dataframe.\n",
    "data = pd.read_csv('C:/Users/S/Documents/PY/increased30featureswopressure.csv', low_memory=False)\n",
    "'''data.loc[data['UUID'] == \"RVTNB1502866560357\", \"attack\"] = 1  \n",
    "data.loc[data['UUID'] != \"RVTNB1502866560357\", \"attack\"] = -1\n",
    "df_majority = data[data['attack']==-1]\n",
    "df_minority = data[data['attack']==1]\n",
    "from sklearn.utils import resample\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=830,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "data = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "#print(data['attack'].value_counts())'''\n",
    "\n",
    "#target=np.array(target)\n",
    "#target = pd.DataFrame(target,columns=['attack'])\n",
    "\n",
    "#data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "categorical_columns=[\"UUID\"]\n",
    "cate_data = data[categorical_columns]\n",
    "\n",
    "#for col in data.columns.values:\n",
    "#    print(col, data[col].unique())\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "def label_encode(cate_data, columns):\n",
    "    for col in columns:\n",
    "        le = LabelEncoder()\n",
    "        col_values_unique = list(cate_data[col].unique())\n",
    "        le_fitted = le.fit(col_values_unique)\n",
    " \n",
    "        col_values = list(cate_data[col].values)\n",
    "        le.classes_\n",
    "        col_values_transformed = le.transform(col_values)\n",
    "        cate_data[col] = col_values_transformed\n",
    " \n",
    "to_be_encoded_cols = cate_data.columns.values\n",
    "label_encode(cate_data, to_be_encoded_cols)\n",
    "display(cate_data.head())\n",
    "target=cate_data['UUID']\n",
    "target=np.array(target)\n",
    "#target = pd.DataFrame(target)\n",
    "#target=target1.values\n",
    "\n",
    "data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "data=pd.concat([data,cate_data], axis=1)\n",
    "data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "\n",
    "# check the shape for sanity checking.\n",
    "data.shape\n",
    "display(data.head())\n",
    "print(\"initial data info\",data.info())\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"data is\",data.shape)\n",
    "from skfeature.function.information_theoretical_based import LCSI\n",
    "from skfeature.function.information_theoretical_based import MRMR\n",
    "\n",
    "from skfeature.utility.entropy_estimators import *\n",
    "import scipy.io\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#scaled_data=data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "scaleddata= pd.DataFrame(scaled_data)\n",
    "scaled_data=np.array(scaled_data)\n",
    "\n",
    "print(scaled_data.shape)\n",
    "print(target.shape)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "#display(target.head())\n",
    "#idx=MRMR.mrmr(scaled_data,target,n_selected_features=50)\n",
    "'''from sklearn import cross_validation\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "correct = 0\n",
    "print(\"scaled data details - \",scaled_data.info())\n",
    "print(\"target data details - \",target.info())\n",
    "for train, test in ss:\n",
    "    #print(scaled_data[train])\n",
    "    #print(target[train])\n",
    "        # obtain the index of each feature on the training set\n",
    "    idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=50)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "    features = scaled_data[:, idx[0:50]]\n",
    "print(features)    '''\n",
    "'''skb= SVC(kernel=\"linear\")\n",
    "rfe = RFE(estimator=skb, n_features_to_select=70)\n",
    "rfe=rfe.fit(scaleddata,target)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "skft = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "for train, test in skft:\n",
    "    X_train,X_test=scaled_data.iloc[train],scaled_data.iloc[test]\n",
    "    Y_train,y_test=target.iloc[train],target.iloc[test]\n",
    "    model1 = svm.OneClassSVM(nu=nu, kernel='rbf', gamma=0.10000000000000001)  \n",
    "    model1.fit(X_train, Y_train)\n",
    "    scores = cross_val_score(model1,X_test,y_test, cv=5, scoring='accuracy')\n",
    "    print(scores)\n",
    "print(scores.mean())'''\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "#rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5,random_state=36851234)\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "'''clf = svm.SVC(decision_function_shape='ovo',kernel='rbf')    # linear SVM\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, cv=skf)\n",
    "grid.fit(scaled_data, target)\n",
    "print(\"The best parameters are %s with a score of %0.2f\"% (grid.best_params_, grid.best_score_))'''\n",
    "plt.figure(figsize=(8, 8))\n",
    "accuracy = plt.subplot(211)\n",
    "box=plt.subplot(211)\n",
    "from sklearn.svm import LinearSVC\n",
    "clf=SVC(kernel='linear')\n",
    "#clf=LinearSVC()\n",
    "rbf=SVC(decision_function_shape='ovo',gamma=0.0001,C=10000000)\n",
    "correct = 0\n",
    "fscoreTotal =0\n",
    "rbf_correct = 0\n",
    "rbf_fscoreTotal =0\n",
    "results=[]\n",
    "for train, test in skf.split(scaled_data,target):\n",
    "        # obtain the index of each feature on the training set\n",
    "    idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=33)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "    features = scaled_data[:, idx[0:33]]\n",
    "        #print(target[train])\n",
    "        # train a classification model with the selected features on the training dataset\n",
    "    clf.fit(features[train], target[train])\n",
    "    #clf1.fit(scaled_data[train],target[train])\n",
    "    rbf.fit(features[train], target[train])\n",
    "        # predict the class labels of test data\n",
    "    y_predict = clf.predict(features[test])\n",
    "    #y_predict = clf1.predict(scaled_data[test])\n",
    "    rbf_y_predict = rbf.predict(features[test])\n",
    "    print(\"metrics\")\n",
    "        # obtain the classification accuracy on the test data\n",
    "    acc = accuracy_score(target[test], y_predict)\n",
    "    rbf_acc = accuracy_score(target[test], rbf_y_predict)\n",
    "    correct = correct + acc\n",
    "    rbf_correct = rbf_correct + rbf_acc\n",
    "    fscore=f1_score(target[test], y_predict,average='weighted')\n",
    "    rbf_fscore=f1_score(target[test], rbf_y_predict,average='weighted')\n",
    "    fscoreTotal=fscoreTotal+fscore\n",
    "    rbf_fscoreTotal=rbf_fscoreTotal+rbf_fscore\n",
    "        #print(\"fsc \",f1_score(target[test], y_predict,average='weighted'))\n",
    "        #print(\"conf mat \",confusion_matrix(target[test],y_predict))\n",
    "        #print(\"ACCURACY: \", (accuracy_score(target[test], y_predict)))\n",
    "    report = classification_report(target[test], y_predict)\n",
    "    print(report)\n",
    "    rbreport = classification_report(target[test], rbf_y_predict)\n",
    "    print(rbreport)\n",
    "    print(\"each loop acc\",acc)\n",
    "    print(\"each loop rbf acc\",rbf_acc)\n",
    "score=float(correct)/5\n",
    "rbfscore=float(rbf_correct)/5\n",
    "results.append(score)\n",
    "results.append(rbfscore)\n",
    "print(\"f1 \",float(fscoreTotal)/5)\n",
    "    # output the average classification accuracy over all 10 folds\n",
    "print(\"Accuracy:\", float(correct)/5)\n",
    "print(\"f1 \",float(rbf_fscoreTotal)/5)\n",
    "    # output the average classification accuracy over all 10 folds\n",
    "print(\"Accuracy:\", float(rbf_correct)/5)\n",
    "\n",
    "accuracy.plot([scaled_data.shape[1], scaled_data.shape[0]],\n",
    "              [score, score], label=\"linear svm\")\n",
    "accuracy.plot([scaled_data.shape[1], scaled_data.shape[0]],\n",
    "              [rbfscore, rbfscore], label=\"rbf svm with grid search C=1000000 and gamma=0.001\")\n",
    "accuracy.set_title(\"Classification accuracy\")\n",
    "accuracy.set_xlim(scaled_data.shape[1], scaled_data.shape[0])\n",
    "accuracy.set_xticks(())\n",
    "accuracy.set_ylim(0.60, 0.80)\n",
    "accuracy.set_ylabel(\"Classification accuracy\")\n",
    "accuracy.legend(loc='best')\n",
    "##svc=SelectKBest(mutual_info_classif, k=50).fit_transform(data,target)\n",
    "#svc = SVC(kernel=\"linear\")\n",
    "#rfe = RFE(estimator=svc, n_features_to_select=10)\n",
    "#rfe.fit(data, target)\n",
    "print(\"here\")\n",
    "box.boxplot(results)\n",
    "box.set_title(\"Classification accuracy\")\n",
    "box.set_xlim(scaled_data.shape[1], scaled_data.shape[0])\n",
    "box.set_xticks(())\n",
    "box.set_ylim(0.60, 0.80)\n",
    "box.set_ylabel(\"Classification accuracy\")\n",
    "box.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UUID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UUID\n",
       "0    48\n",
       "1    48\n",
       "2    48\n",
       "3    48\n",
       "4    48"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aLN1</th>\n",
       "      <th>a.2</th>\n",
       "      <th>aLN3</th>\n",
       "      <th>at4</th>\n",
       "      <th>ai5</th>\n",
       "      <th>ae6</th>\n",
       "      <th>aLN7</th>\n",
       "      <th>a58</th>\n",
       "      <th>aLN9</th>\n",
       "      <th>aSH10</th>\n",
       "      <th>...</th>\n",
       "      <th>du2o12</th>\n",
       "      <th>du2a13</th>\n",
       "      <th>du2n14</th>\n",
       "      <th>du2n15</th>\n",
       "      <th>avgdu</th>\n",
       "      <th>avgud</th>\n",
       "      <th>avgdd</th>\n",
       "      <th>avguu</th>\n",
       "      <th>avdu2</th>\n",
       "      <th>avga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2382</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>973</td>\n",
       "      <td>37.875</td>\n",
       "      <td>24.466667</td>\n",
       "      <td>56.800000</td>\n",
       "      <td>55.866667</td>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.004412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>3015</td>\n",
       "      <td>1081</td>\n",
       "      <td>37.625</td>\n",
       "      <td>31.933333</td>\n",
       "      <td>64.066667</td>\n",
       "      <td>63.266667</td>\n",
       "      <td>95.400000</td>\n",
       "      <td>0.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3015</td>\n",
       "      <td>2361</td>\n",
       "      <td>1918</td>\n",
       "      <td>884</td>\n",
       "      <td>64.125</td>\n",
       "      <td>453.733333</td>\n",
       "      <td>515.933333</td>\n",
       "      <td>513.133333</td>\n",
       "      <td>575.333333</td>\n",
       "      <td>0.008333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2361</td>\n",
       "      <td>1918</td>\n",
       "      <td>1438</td>\n",
       "      <td>827</td>\n",
       "      <td>63.250</td>\n",
       "      <td>347.733333</td>\n",
       "      <td>407.733333</td>\n",
       "      <td>406.400000</td>\n",
       "      <td>466.400000</td>\n",
       "      <td>0.008211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2382</td>\n",
       "      <td>2302</td>\n",
       "      <td>670740857</td>\n",
       "      <td>973</td>\n",
       "      <td>69.375</td>\n",
       "      <td>-9.133333</td>\n",
       "      <td>56.800000</td>\n",
       "      <td>55.866667</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>0.009804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aLN1       a.2      aLN3       at4       ai5       ae6      aLN7  a58  \\\n",
       "0  0.017647  0.015686  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "1  0.015686  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "2  0.017647  0.015686  0.015686  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "3  0.015686  0.015686  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "4  0.017647  0.015686  0.015686  0.019608  0.015686  0.017647  0.017647  0.0   \n",
       "\n",
       "   aLN9  aSH10    ...     du2o12     du2a13     du2n14  du2n15   avgdu  \\\n",
       "0   0.0    0.0    ...       2382       2302  670740857     973  37.875   \n",
       "1   0.0    0.0    ...       2302  670740857       3015    1081  37.625   \n",
       "2   0.0    0.0    ...       3015       2361       1918     884  64.125   \n",
       "3   0.0    0.0    ...       2361       1918       1438     827  63.250   \n",
       "4   0.0    0.0    ...       2382       2302  670740857     973  69.375   \n",
       "\n",
       "        avgud       avgdd       avguu       avdu2      avga  \n",
       "0   24.466667   56.800000   55.866667   88.200000  0.004412  \n",
       "1   31.933333   64.066667   63.266667   95.400000  0.004167  \n",
       "2  453.733333  515.933333  513.133333  575.333333  0.008333  \n",
       "3  347.733333  407.733333  406.400000  466.400000  0.008211  \n",
       "4   -9.133333   56.800000   55.866667  121.800000  0.009804  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2310 entries, 0 to 2309\n",
      "Columns: 130 entries, aLN1 to avga\n",
      "dtypes: float64(54), int64(76)\n",
      "memory usage: 2.3 MB\n",
      "initial data info None\n",
      "data is (2310, 130)\n",
      "(2310, 130)\n",
      "(2310,)\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      1.00      0.86         6\n",
      "          1       0.86      1.00      0.92         6\n",
      "          2       0.50      1.00      0.67         6\n",
      "          3       1.00      1.00      1.00         6\n",
      "          4       0.75      1.00      0.86         6\n",
      "          5       1.00      1.00      1.00         6\n",
      "          6       1.00      1.00      1.00         6\n",
      "          7       1.00      0.83      0.91         6\n",
      "          8       0.40      0.67      0.50         6\n",
      "          9       0.71      0.83      0.77         6\n",
      "         10       0.44      0.67      0.53         6\n",
      "         11       0.86      1.00      0.92         6\n",
      "         12       1.00      1.00      1.00         6\n",
      "         13       1.00      1.00      1.00         6\n",
      "         14       0.60      0.50      0.55         6\n",
      "         15       1.00      1.00      1.00         6\n",
      "         16       1.00      0.67      0.80         6\n",
      "         17       1.00      1.00      1.00         6\n",
      "         18       0.67      0.67      0.67         6\n",
      "         19       1.00      0.83      0.91         6\n",
      "         20       1.00      0.83      0.91         6\n",
      "         21       0.50      0.83      0.62         6\n",
      "         22       0.80      0.67      0.73         6\n",
      "         23       1.00      0.83      0.91         6\n",
      "         24       0.80      0.67      0.73         6\n",
      "         25       0.86      1.00      0.92         6\n",
      "         26       0.67      1.00      0.80         6\n",
      "         27       0.67      0.33      0.44         6\n",
      "         28       1.00      0.50      0.67         6\n",
      "         29       1.00      0.67      0.80         6\n",
      "         30       0.71      0.83      0.77         6\n",
      "         31       0.62      0.83      0.71         6\n",
      "         32       1.00      0.83      0.91         6\n",
      "         33       1.00      0.67      0.80         6\n",
      "         34       0.86      1.00      0.92         6\n",
      "         35       0.71      0.83      0.77         6\n",
      "         36       0.80      0.67      0.73         6\n",
      "         37       0.83      0.83      0.83         6\n",
      "         38       1.00      1.00      1.00         6\n",
      "         39       0.43      0.50      0.46         6\n",
      "         40       0.50      0.33      0.40         6\n",
      "         41       1.00      0.83      0.91         6\n",
      "         42       0.86      1.00      0.92         6\n",
      "         43       0.50      0.17      0.25         6\n",
      "         44       0.60      0.50      0.55         6\n",
      "         45       0.86      1.00      0.92         6\n",
      "         46       0.62      0.83      0.71         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       1.00      0.83      0.91         6\n",
      "         49       0.62      0.83      0.71         6\n",
      "         50       0.43      0.50      0.46         6\n",
      "         51       0.00      0.00      0.00         6\n",
      "         52       1.00      0.67      0.80         6\n",
      "         53       1.00      0.50      0.67         6\n",
      "         54       1.00      0.33      0.50         6\n",
      "         55       0.71      0.83      0.77         6\n",
      "         56       0.86      1.00      0.92         6\n",
      "         57       1.00      1.00      1.00         6\n",
      "         58       0.75      1.00      0.86         6\n",
      "         59       1.00      1.00      1.00         6\n",
      "         60       0.86      1.00      0.92         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       1.00      0.33      0.50         6\n",
      "         63       0.67      0.67      0.67         6\n",
      "         64       0.86      1.00      0.92         6\n",
      "         65       0.40      0.67      0.50         6\n",
      "         66       0.80      0.67      0.73         6\n",
      "         67       0.60      0.50      0.55         6\n",
      "         68       1.00      1.00      1.00         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       0.80      0.67      0.73         6\n",
      "         71       0.80      0.67      0.73         6\n",
      "         72       0.67      0.67      0.67         6\n",
      "         73       0.86      1.00      0.92         6\n",
      "         74       0.86      1.00      0.92         6\n",
      "         75       0.75      0.50      0.60         6\n",
      "         76       0.67      0.67      0.67         6\n",
      "\n",
      "avg / total       0.80      0.78      0.77       462\n",
      "\n",
      "each loop acc 0.781385281385\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      1.00      0.92         6\n",
      "          1       1.00      1.00      1.00         6\n",
      "          2       0.83      0.83      0.83         6\n",
      "          3       1.00      0.83      0.91         6\n",
      "          4       1.00      1.00      1.00         6\n",
      "          5       0.83      0.83      0.83         6\n",
      "          6       1.00      1.00      1.00         6\n",
      "          7       1.00      1.00      1.00         6\n",
      "          8       0.62      0.83      0.71         6\n",
      "          9       1.00      1.00      1.00         6\n",
      "         10       0.80      0.67      0.73         6\n",
      "         11       0.75      1.00      0.86         6\n",
      "         12       0.86      1.00      0.92         6\n",
      "         13       0.67      1.00      0.80         6\n",
      "         14       0.86      1.00      0.92         6\n",
      "         15       1.00      0.83      0.91         6\n",
      "         16       0.71      0.83      0.77         6\n",
      "         17       0.80      0.67      0.73         6\n",
      "         18       0.71      0.83      0.77         6\n",
      "         19       0.71      0.83      0.77         6\n",
      "         20       1.00      0.83      0.91         6\n",
      "         21       1.00      0.50      0.67         6\n",
      "         22       0.83      0.83      0.83         6\n",
      "         23       1.00      1.00      1.00         6\n",
      "         24       0.38      0.50      0.43         6\n",
      "         25       1.00      1.00      1.00         6\n",
      "         26       1.00      0.83      0.91         6\n",
      "         27       0.40      0.33      0.36         6\n",
      "         28       0.83      0.83      0.83         6\n",
      "         29       1.00      0.67      0.80         6\n",
      "         30       1.00      1.00      1.00         6\n",
      "         31       0.86      1.00      0.92         6\n",
      "         32       1.00      1.00      1.00         6\n",
      "         33       1.00      0.83      0.91         6\n",
      "         34       0.80      0.67      0.73         6\n",
      "         35       0.86      1.00      0.92         6\n",
      "         36       1.00      0.83      0.91         6\n",
      "         37       1.00      1.00      1.00         6\n",
      "         38       0.83      0.83      0.83         6\n",
      "         39       0.67      0.67      0.67         6\n",
      "         40       0.75      0.50      0.60         6\n",
      "         41       1.00      1.00      1.00         6\n",
      "         42       0.67      0.67      0.67         6\n",
      "         43       1.00      1.00      1.00         6\n",
      "         44       0.67      0.67      0.67         6\n",
      "         45       0.80      0.67      0.73         6\n",
      "         46       0.60      0.50      0.55         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       0.71      0.83      0.77         6\n",
      "         49       0.80      0.67      0.73         6\n",
      "         50       0.75      0.50      0.60         6\n",
      "         51       0.67      0.67      0.67         6\n",
      "         52       0.71      0.83      0.77         6\n",
      "         53       0.86      1.00      0.92         6\n",
      "         54       0.71      0.83      0.77         6\n",
      "         55       0.50      0.50      0.50         6\n",
      "         56       1.00      1.00      1.00         6\n",
      "         57       0.57      0.67      0.62         6\n",
      "         58       0.86      1.00      0.92         6\n",
      "         59       1.00      1.00      1.00         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       0.67      0.67      0.67         6\n",
      "         63       1.00      0.83      0.91         6\n",
      "         64       0.86      1.00      0.92         6\n",
      "         65       1.00      0.50      0.67         6\n",
      "         66       0.75      1.00      0.86         6\n",
      "         67       0.80      0.67      0.73         6\n",
      "         68       0.83      0.83      0.83         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       1.00      0.83      0.91         6\n",
      "         71       0.75      1.00      0.86         6\n",
      "         72       0.83      0.83      0.83         6\n",
      "         73       0.80      0.67      0.73         6\n",
      "         74       1.00      0.50      0.67         6\n",
      "         75       0.44      0.67      0.53         6\n",
      "         76       0.71      0.83      0.77         6\n",
      "\n",
      "avg / total       0.84      0.82      0.82       462\n",
      "\n",
      "each loop acc 0.824675324675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      1.00      0.92         6\n",
      "          1       1.00      1.00      1.00         6\n",
      "          2       1.00      0.83      0.91         6\n",
      "          3       1.00      1.00      1.00         6\n",
      "          4       0.86      1.00      0.92         6\n",
      "          5       1.00      1.00      1.00         6\n",
      "          6       1.00      1.00      1.00         6\n",
      "          7       1.00      0.83      0.91         6\n",
      "          8       0.57      0.67      0.62         6\n",
      "          9       0.75      0.50      0.60         6\n",
      "         10       0.67      1.00      0.80         6\n",
      "         11       0.83      0.83      0.83         6\n",
      "         12       1.00      1.00      1.00         6\n",
      "         13       1.00      1.00      1.00         6\n",
      "         14       0.80      0.67      0.73         6\n",
      "         15       1.00      0.67      0.80         6\n",
      "         16       0.83      0.83      0.83         6\n",
      "         17       0.80      0.67      0.73         6\n",
      "         18       1.00      0.50      0.67         6\n",
      "         19       1.00      1.00      1.00         6\n",
      "         20       0.83      0.83      0.83         6\n",
      "         21       1.00      0.67      0.80         6\n",
      "         22       1.00      0.83      0.91         6\n",
      "         23       0.86      1.00      0.92         6\n",
      "         24       1.00      0.33      0.50         6\n",
      "         25       0.80      0.67      0.73         6\n",
      "         26       1.00      1.00      1.00         6\n",
      "         27       0.50      0.17      0.25         6\n",
      "         28       0.62      0.83      0.71         6\n",
      "         29       1.00      1.00      1.00         6\n",
      "         30       0.83      0.83      0.83         6\n",
      "         31       0.62      0.83      0.71         6\n",
      "         32       0.86      1.00      0.92         6\n",
      "         33       0.75      0.50      0.60         6\n",
      "         34       0.86      1.00      0.92         6\n",
      "         35       0.50      0.33      0.40         6\n",
      "         36       0.67      1.00      0.80         6\n",
      "         37       0.83      0.83      0.83         6\n",
      "         38       1.00      0.67      0.80         6\n",
      "         39       0.67      1.00      0.80         6\n",
      "         40       0.50      0.67      0.57         6\n",
      "         41       0.86      1.00      0.92         6\n",
      "         42       0.83      0.83      0.83         6\n",
      "         43       1.00      0.83      0.91         6\n",
      "         44       0.83      0.83      0.83         6\n",
      "         45       1.00      0.67      0.80         6\n",
      "         46       0.44      0.67      0.53         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       0.57      0.67      0.62         6\n",
      "         49       0.71      0.83      0.77         6\n",
      "         50       0.67      1.00      0.80         6\n",
      "         51       0.50      0.17      0.25         6\n",
      "         52       0.83      0.83      0.83         6\n",
      "         53       1.00      0.83      0.91         6\n",
      "         54       1.00      0.50      0.67         6\n",
      "         55       0.38      0.83      0.53         6\n",
      "         56       0.83      0.83      0.83         6\n",
      "         57       1.00      1.00      1.00         6\n",
      "         58       0.62      0.83      0.71         6\n",
      "         59       1.00      1.00      1.00         6\n",
      "         60       1.00      0.83      0.91         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       0.50      0.33      0.40         6\n",
      "         63       1.00      1.00      1.00         6\n",
      "         64       0.80      0.67      0.73         6\n",
      "         65       0.40      0.33      0.36         6\n",
      "         66       0.83      0.83      0.83         6\n",
      "         67       0.86      1.00      0.92         6\n",
      "         68       1.00      0.83      0.91         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       0.57      0.67      0.62         6\n",
      "         71       0.86      1.00      0.92         6\n",
      "         72       0.75      0.50      0.60         6\n",
      "         73       0.86      1.00      0.92         6\n",
      "         74       0.60      1.00      0.75         6\n",
      "         75       0.20      0.17      0.18         6\n",
      "         76       0.62      0.83      0.71         6\n",
      "\n",
      "avg / total       0.81      0.79      0.79       462\n",
      "\n",
      "each loop acc 0.794372294372\n",
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.83      0.83         6\n",
      "          1       0.86      1.00      0.92         6\n",
      "          2       0.50      0.83      0.62         6\n",
      "          3       1.00      1.00      1.00         6\n",
      "          4       0.57      0.67      0.62         6\n",
      "          5       0.86      1.00      0.92         6\n",
      "          6       1.00      0.50      0.67         6\n",
      "          7       0.80      0.67      0.73         6\n",
      "          8       1.00      1.00      1.00         6\n",
      "          9       0.80      0.67      0.73         6\n",
      "         10       0.75      0.50      0.60         6\n",
      "         11       1.00      1.00      1.00         6\n",
      "         12       0.80      0.67      0.73         6\n",
      "         13       1.00      1.00      1.00         6\n",
      "         14       0.50      0.50      0.50         6\n",
      "         15       0.75      1.00      0.86         6\n",
      "         16       0.62      0.83      0.71         6\n",
      "         17       0.80      0.67      0.73         6\n",
      "         18       0.83      0.83      0.83         6\n",
      "         19       1.00      0.83      0.91         6\n",
      "         20       0.75      1.00      0.86         6\n",
      "         21       1.00      0.83      0.91         6\n",
      "         22       1.00      0.83      0.91         6\n",
      "         23       0.83      0.83      0.83         6\n",
      "         24       1.00      0.33      0.50         6\n",
      "         25       0.50      1.00      0.67         6\n",
      "         26       0.86      1.00      0.92         6\n",
      "         27       1.00      0.50      0.67         6\n",
      "         28       0.67      1.00      0.80         6\n",
      "         29       0.50      0.67      0.57         6\n",
      "         30       1.00      0.67      0.80         6\n",
      "         31       1.00      0.83      0.91         6\n",
      "         32       1.00      1.00      1.00         6\n",
      "         33       0.50      0.67      0.57         6\n",
      "         34       1.00      0.67      0.80         6\n",
      "         35       0.38      0.50      0.43         6\n",
      "         36       1.00      0.67      0.80         6\n",
      "         37       1.00      0.33      0.50         6\n",
      "         38       1.00      1.00      1.00         6\n",
      "         39       0.67      0.33      0.44         6\n",
      "         40       0.60      0.50      0.55         6\n",
      "         41       0.71      0.83      0.77         6\n",
      "         42       1.00      0.50      0.67         6\n",
      "         43       0.75      1.00      0.86         6\n",
      "         44       1.00      0.67      0.80         6\n",
      "         45       1.00      1.00      1.00         6\n",
      "         46       0.43      0.50      0.46         6\n",
      "         47       1.00      0.50      0.67         6\n",
      "         48       0.83      0.83      0.83         6\n",
      "         49       0.45      0.83      0.59         6\n",
      "         50       0.57      0.67      0.62         6\n",
      "         51       0.50      0.50      0.50         6\n",
      "         52       0.75      1.00      0.86         6\n",
      "         53       0.67      1.00      0.80         6\n",
      "         54       0.83      0.83      0.83         6\n",
      "         55       0.86      1.00      0.92         6\n",
      "         56       1.00      0.67      0.80         6\n",
      "         57       0.80      0.67      0.73         6\n",
      "         58       0.67      0.67      0.67         6\n",
      "         59       0.86      1.00      0.92         6\n",
      "         60       1.00      1.00      1.00         6\n",
      "         61       1.00      0.83      0.91         6\n",
      "         62       0.75      0.50      0.60         6\n",
      "         63       1.00      0.83      0.91         6\n",
      "         64       1.00      0.83      0.91         6\n",
      "         65       0.71      0.83      0.77         6\n",
      "         66       0.50      0.50      0.50         6\n",
      "         67       0.60      0.50      0.55         6\n",
      "         68       0.86      1.00      0.92         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       0.50      0.50      0.50         6\n",
      "         71       1.00      1.00      1.00         6\n",
      "         72       1.00      0.83      0.91         6\n",
      "         73       0.75      0.50      0.60         6\n",
      "         74       1.00      1.00      1.00         6\n",
      "         75       0.55      1.00      0.71         6\n",
      "         76       0.86      1.00      0.92         6\n",
      "\n",
      "avg / total       0.81      0.77      0.77       462\n",
      "\n",
      "each loop acc 0.772727272727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.33      0.50         6\n",
      "          1       1.00      1.00      1.00         6\n",
      "          2       0.83      0.83      0.83         6\n",
      "          3       1.00      1.00      1.00         6\n",
      "          4       1.00      1.00      1.00         6\n",
      "          5       1.00      1.00      1.00         6\n",
      "          6       1.00      1.00      1.00         6\n",
      "          7       1.00      1.00      1.00         6\n",
      "          8       0.50      0.67      0.57         6\n",
      "          9       0.80      0.67      0.73         6\n",
      "         10       0.83      0.83      0.83         6\n",
      "         11       1.00      1.00      1.00         6\n",
      "         12       1.00      1.00      1.00         6\n",
      "         13       1.00      1.00      1.00         6\n",
      "         14       1.00      1.00      1.00         6\n",
      "         15       0.60      0.50      0.55         6\n",
      "         16       1.00      0.67      0.80         6\n",
      "         17       0.86      1.00      0.92         6\n",
      "         18       0.62      0.83      0.71         6\n",
      "         19       1.00      1.00      1.00         6\n",
      "         20       1.00      0.83      0.91         6\n",
      "         21       0.67      0.67      0.67         6\n",
      "         22       0.71      0.83      0.77         6\n",
      "         23       0.83      0.83      0.83         6\n",
      "         24       0.80      0.67      0.73         6\n",
      "         25       0.80      0.67      0.73         6\n",
      "         26       0.45      0.83      0.59         6\n",
      "         27       0.00      0.00      0.00         6\n",
      "         28       0.44      0.67      0.53         6\n",
      "         29       0.62      0.83      0.71         6\n",
      "         30       0.75      1.00      0.86         6\n",
      "         31       0.75      0.50      0.60         6\n",
      "         32       1.00      1.00      1.00         6\n",
      "         33       0.62      0.83      0.71         6\n",
      "         34       1.00      1.00      1.00         6\n",
      "         35       0.83      0.83      0.83         6\n",
      "         36       0.60      0.50      0.55         6\n",
      "         37       0.71      0.83      0.77         6\n",
      "         38       1.00      1.00      1.00         6\n",
      "         39       0.22      0.33      0.27         6\n",
      "         40       0.71      0.83      0.77         6\n",
      "         41       0.67      0.67      0.67         6\n",
      "         42       1.00      1.00      1.00         6\n",
      "         43       0.62      0.83      0.71         6\n",
      "         44       0.71      0.83      0.77         6\n",
      "         45       0.71      0.83      0.77         6\n",
      "         46       0.75      0.50      0.60         6\n",
      "         47       1.00      1.00      1.00         6\n",
      "         48       0.83      0.83      0.83         6\n",
      "         49       0.62      0.83      0.71         6\n",
      "         50       0.46      1.00      0.63         6\n",
      "         51       0.21      0.50      0.30         6\n",
      "         52       1.00      0.50      0.67         6\n",
      "         53       1.00      0.83      0.91         6\n",
      "         54       1.00      0.83      0.91         6\n",
      "         55       0.40      0.33      0.36         6\n",
      "         56       1.00      1.00      1.00         6\n",
      "         57       0.75      1.00      0.86         6\n",
      "         58       0.75      0.50      0.60         6\n",
      "         59       1.00      1.00      1.00         6\n",
      "         60       0.75      0.50      0.60         6\n",
      "         61       1.00      1.00      1.00         6\n",
      "         62       1.00      0.50      0.67         6\n",
      "         63       1.00      0.83      0.91         6\n",
      "         64       0.86      1.00      0.92         6\n",
      "         65       0.67      0.33      0.44         6\n",
      "         66       0.86      1.00      0.92         6\n",
      "         67       1.00      0.83      0.91         6\n",
      "         68       0.80      0.67      0.73         6\n",
      "         69       1.00      1.00      1.00         6\n",
      "         70       0.83      0.83      0.83         6\n",
      "         71       1.00      1.00      1.00         6\n",
      "         72       0.83      0.83      0.83         6\n",
      "         73       1.00      0.17      0.29         6\n",
      "         74       1.00      1.00      1.00         6\n",
      "         75       0.50      0.17      0.25         6\n",
      "         76       0.83      0.83      0.83         6\n",
      "\n",
      "avg / total       0.81      0.78      0.78       462\n",
      "\n",
      "each loop acc 0.781385281385\n",
      "random f1  0.7861144602385406\n",
      "random Accuracy: 0.7909090909090908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\S\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import lsanomaly\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn import utils  \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "\n",
    "# import the CSV from http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "# this will return a pandas dataframe.\n",
    "data = pd.read_csv('C:/Users/S/Documents/PY/increased30featureswopressure.csv', low_memory=False)\n",
    "'''data.loc[data['UUID'] == \"RVTNB1502866560357\", \"attack\"] = 1  \n",
    "data.loc[data['UUID'] != \"RVTNB1502866560357\", \"attack\"] = -1\n",
    "df_majority = data[data['attack']==-1]\n",
    "df_minority = data[data['attack']==1]\n",
    "from sklearn.utils import resample\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=830,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "data = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "#print(data['attack'].value_counts())'''\n",
    "\n",
    "#target=np.array(target)\n",
    "#target = pd.DataFrame(target,columns=['attack'])\n",
    "\n",
    "#data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "categorical_columns=[\"UUID\"]\n",
    "cate_data = data[categorical_columns]\n",
    "\n",
    "#for col in data.columns.values:\n",
    "#    print(col, data[col].unique())\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "def label_encode(cate_data, columns):\n",
    "    for col in columns:\n",
    "        le = LabelEncoder()\n",
    "        col_values_unique = list(cate_data[col].unique())\n",
    "        le_fitted = le.fit(col_values_unique)\n",
    " \n",
    "        col_values = list(cate_data[col].values)\n",
    "        le.classes_\n",
    "        col_values_transformed = le.transform(col_values)\n",
    "        cate_data[col] = col_values_transformed\n",
    " \n",
    "to_be_encoded_cols = cate_data.columns.values\n",
    "label_encode(cate_data, to_be_encoded_cols)\n",
    "display(cate_data.head())\n",
    "target=cate_data['UUID']\n",
    "target=np.array(target)\n",
    "#target = pd.DataFrame(target)\n",
    "#target=target1.values\n",
    "\n",
    "data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "data=pd.concat([data,cate_data], axis=1)\n",
    "data.drop([\"UUID\"], axis=1, inplace=True)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "\n",
    "# check the shape for sanity checking.\n",
    "data.shape\n",
    "display(data.head())\n",
    "print(\"initial data info\",data.info())\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"data is\",data.shape)\n",
    "from skfeature.function.information_theoretical_based import LCSI\n",
    "from skfeature.function.information_theoretical_based import MRMR\n",
    "\n",
    "from skfeature.utility.entropy_estimators import *\n",
    "import scipy.io\n",
    "import csv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#scaled_data=data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "scaleddata= pd.DataFrame(scaled_data)\n",
    "scaled_data=np.array(scaled_data)\n",
    "\n",
    "print(scaled_data.shape)\n",
    "print(target.shape)\n",
    "#display(scaled_data.head())\n",
    "\n",
    "#display(target.head())\n",
    "#idx=MRMR.mrmr(scaled_data,target,n_selected_features=50)\n",
    "'''from sklearn import cross_validation\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "correct = 0\n",
    "print(\"scaled data details - \",scaled_data.info())\n",
    "print(\"target data details - \",target.info())\n",
    "for train, test in ss:\n",
    "    #print(scaled_data[train])\n",
    "    #print(target[train])\n",
    "        # obtain the index of each feature on the training set\n",
    "    idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=50)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "    features = scaled_data[:, idx[0:50]]\n",
    "print(features)    '''\n",
    "'''skb= SVC(kernel=\"linear\")\n",
    "rfe = RFE(estimator=skb, n_features_to_select=70)\n",
    "rfe=rfe.fit(scaleddata,target)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "skft = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "for train, test in skft:\n",
    "    X_train,X_test=scaled_data.iloc[train],scaled_data.iloc[test]\n",
    "    Y_train,y_test=target.iloc[train],target.iloc[test]\n",
    "    model1 = svm.OneClassSVM(nu=nu, kernel='rbf', gamma=0.10000000000000001)  \n",
    "    model1.fit(X_train, Y_train)\n",
    "    scores = cross_val_score(model1,X_test,y_test, cv=5, scoring='accuracy')\n",
    "    print(scores)\n",
    "print(scores.mean())'''\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ss = cross_validation.KFold(5, n_folds=5, shuffle=True)\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "#rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5,random_state=36851234)\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=36851234)\n",
    "''''from sklearn.model_selection import GridSearchCV\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "clf = svm.SVC(decision_function_shape='ovo',kernel='rbf')    # linear SVM\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, cv=skf)\n",
    "grid.fit(scaled_data, target)\n",
    "print(\"The best parameters are %s with a score of %0.2f\"% (grid.best_params_, grid.best_score_))'''\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "clf=RandomForestClassifier(n_estimators=1,class_weight='balanced')\n",
    "\n",
    "\n",
    "correct = 0\n",
    "fscoreTotal =0\n",
    "\n",
    "results=[]\n",
    "for train, test in skf.split(scaled_data,target):\n",
    "        # obtain the index of each feature on the training set\n",
    "    idx,_,_ = MRMR.mrmr(scaled_data[train], target[train], n_selected_features=33)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "    features = scaled_data[:, idx[0:33]]\n",
    "    \n",
    "        # train a classification model with the selected features on the training dataset\n",
    "    clf.fit(features[train], target[train])\n",
    "    #clf1.fit(scaled_data[train],target[train])\n",
    "    \n",
    "        # predict the class labels of test data\n",
    "    y_predict = clf.predict(features[test])\n",
    "    #y_predict = clf1.predict(scaled_data[test])\n",
    "    \n",
    "    print(\"metrics\")\n",
    "        # obtain the classification accuracy on the test data\n",
    "    acc = accuracy_score(target[test], y_predict)\n",
    "    \n",
    "    correct = correct + acc\n",
    "    \n",
    "    fscore=f1_score(target[test], y_predict,average='weighted')\n",
    "    \n",
    "    fscoreTotal=fscoreTotal+fscore\n",
    "    \n",
    "        #print(\"fsc \",f1_score(target[test], y_predict,average='weighted'))\n",
    "        #print(\"conf mat \",confusion_matrix(target[test],y_predict))\n",
    "        #print(\"ACCURACY: \", (accuracy_score(target[test], y_predict)))\n",
    "    report = classification_report(target[test], y_predict)\n",
    "    print(report)\n",
    "    \n",
    "    print(\"each loop acc\",acc)\n",
    "   \n",
    "score=float(correct)/5\n",
    "\n",
    "results.append(score)\n",
    "\n",
    "print(\"random f1 \",float(fscoreTotal)/5)\n",
    "    # output the average classification accuracy over all 10 folds\n",
    "print(\"random Accuracy:\", float(correct)/5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
